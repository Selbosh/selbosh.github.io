---
title: Advent of Code 2021
date: '2021-12-01T09:00:00+00:00'
slug: advent-2021
categories: ['R']
images: ['/img/2020/adventofcode.jpg']
---

It's that time of year again.
And not just for [Secret Santa](/2016/12/07/santa/)---it's time for the [Advent of Code](https://adventofcode.com/), a series of programming
puzzles in the lead-up to Christmas.

I'm doing the 2021 challenge in R---in the form of an open-source [R package](https://github.com/Selbosh/adventofcode2021), to demonstrate a [test-driven](https://personalpages.manchester.ac.uk/staff/david.selby/rthritis/2021-11-19-unittesting/) workflow.

<div style="text-align:center;">
  <div class="github-card" data-github="Selbosh/adventofcode2021" data-width="400" data-height="" data-theme="default" style="display:block; margin:0 auto;"></div>
</div>
<script src="//cdn.jsdelivr.net/github-cards/latest/widget.js"></script>

Each puzzle description typically comes with a few simple examples of inputs and outputs.
We can use these to define expectations for unit tests with the [**testthat**](https://testthat.r-lib.org/) package.
Once a function passes the unit tests, it should be ready to try with the main puzzle input.

Check my [**adventofcode2021**](https://github.com/Selbosh/adventofcode2021) repository on GitHub for the latest.

```r
remotes::install_github('Selbosh/adventofcode2021')
```

1. [Sonar Sweep](#day1)
2. [Dive!](#day2)
3. [Binary Diagnostic](#day3)
4. [Giant Squid](#day4)
5. [Hydrothermal Venture](#day5)
6. [Lanternfish](#day6)
7. [The Treachery of Whales](#day7)
8. [Seven Segment Search](#day8)
9. [Smoke Basin](#day9)
10. [Syntax Scoring](#day10)
11. [Dumbo Octopus](#day11)
12. [Passage Pathing](#day12)
13. [Transparent Origami](#day13)
14. [Extended Polymerization](#day14)
15. [Chiton](#day15)
16. [Packet Decoder](#day16)
17. [Trick Shot](#day17)
18. [Snailfish](#day18)

## Day 1 - [Sonar Sweep](https://adventofcode.com/2021/day/1) {#day1}

### Increases

To count the number of times elements are increasing in a vector it's as simple as

```r
depths <- c(199, 200, 208, 210, 200, 207, 240, 269, 260, 263)
sum(diff(depths) > 0)
```

```
## [1] 7
```

for which I defined a function called [`increases`](https://github.com/Selbosh/adventofcode2021/blob/main/R/day01.R#L91-L93).

### Rolling sum

For part two, we first want to calculate the three-depth moving sum, then we count the increases as in part one.
There are plenty of solutions in external R packages for getting lagged (and leading) vectors, for instance `dplyr::lag()` and `dplyr::lead()`: 

```r
depths + dplyr::lead(depths) + dplyr::lead(depths, 2)
```

```
##  [1] 607 618 618 617 647 716 769 792  NA  NA
```

Or you could even calculate the rolling sum using a pre-made solution in **zoo** (Z's Ordered Observations, a time-series analysis package).

```r
zoo::rollsum(depths, 3)
```

```
## [1] 607 618 618 617 647 716 769 792
```

To avoid loading any external packages at this stage, I defined my own base R function called [`rolling_sum()`](https://github.com/Selbosh/adventofcode2021/blob/main/R/day01.R#L99-L101), which uses `tail` and `head` with negative lengths to omit the first and last elements of the vector:

```r
head(depths, -2) + head(tail(depths, -1), -1) + tail(depths, -2)
```

```
## [1] 607 618 618 617 647 716 769 792
```

As [David Schoch points out](https://twitter.com/schochastics/status/1466062839077027845), you can just use the `lag` argument of `diff` to make this entire puzzle into a one-liner:

```r
sapply(c(1, 3), \(lag) sum(diff(depths, lag) > 0))
```

```
## [1] 7 5
```

## Day 2 - [Dive!](https://adventofcode.com/2021/day/2) {#day2}

### Depth sum

Read in the input as a two-column data frame using `read.table()`.
I gave mine nice column names, `cmd` and `value`, but this isn't essential.

Then take advantage of the fact that `TRUE == 1` and `FALSE == 0` to make a mathematical `ifelse`-type statement for the horizontal and vertical movements.
In my R package, this is implemented as a function called [`dive()`](https://github.com/Selbosh/adventofcode2021/blob/main/R/day02.R#L70-L76):

```r
x <- (cmd == 'forward') * value
y <- ((cmd == 'down') - (cmd == 'up')) * value
sum(x) * sum(y)
```

### Cumulative depth sum

Part two is much like part one, but now `y` represents (change in) aim, and (change in) depth is derived from that.
Don't forget the function `cumsum()`, which can save you writing a loop!
Here is the body of my function [`dive2()`](https://github.com/Selbosh/adventofcode2021/blob/main/R/day02.R#L80-L87):

```r
x <- (cmd == 'forward') * value
y <- ((cmd == 'down') - (cmd == 'up')) * value
depth <- cumsum(y) * x
sum(x) * sum(depth)
```

## Day 3 - [Binary Diagnostic](https://adventofcode.com/2021/day/2) {#day3}

### Power consumption

There are a few different ways you could approach part one, but my approach was first to read in the data as a data frame of binary integers using the function `read.fwf()`.
Then, find the most common value in each column using the base function `colMeans()` and rounding the result.

According to the instructions, in the event of a tie you should take 1 to be the most common digit.
Although this is familiar to real life---0.5 rounds up to 1---computers [don't work this way](https://en.wikipedia.org/wiki/Rounding#Round_half_to_even): R rounds to even instead (see `?round`).
Because zero is even, that means `round(0.5)` yields 0.
To get around this, add 1 before rounding, then subtract it again.

My function [`power_consumption()`](https://github.com/Selbosh/adventofcode2021/blob/main/R/day03.R#L78-L81), which once again takes advantage of `TRUE` being equivalent to 1 and `FALSE` to 0:

```r
common <- round(colMeans(x) + 1) - 1
binary_to_int(common) * binary_to_int(!common)
```

To convert a vector of binary digits to decimal, I use the following [utility function](https://github.com/Selbosh/adventofcode2021/blob/main/R/day03.R#L104-L106):

```r
binary_to_int <- function(x) {
  sum(x * 2 ^ rev(seq_along(x) - 1))
}
```

However, if using a string representation then there's a handy function in base R called `strtoi()` that you could also use for this ([thanks to Riinu Pius for that tip](https://twitter.com/_Riinu_/status/1466681283887648769)).

### Life support

Part two finds the common digits in a successively decreasing set of binary numbers.
A loop is appropriate here, since we can halt once there is only one number left.
As this loop will only run (at most) 12 times in total, it shouldn't be too slow in R.

Function [`life_support()`](https://github.com/Selbosh/adventofcode2021/blob/main/R/day03.R#L85-L98):

```r
life_support <- function(x) {
  oxygen <- co2 <- x
  for (j in 1:ncol(x)) {
    if (nrow(oxygen) > 1) {
      common <- most_common(oxygen)
      oxygen <- oxygen[oxygen[, j] == common[j], ]
    }
    if (nrow(co2) > 1) {
      common <- most_common(co2)
      co2 <- co2[co2[, j] != common[j], ]
    }
  }
  binary_to_int(oxygen) * binary_to_int(co2)
}
```

There might be cleverer ways of doing this.

## Day 4 - [Giant Squid](https://adventofcode.com/2021/day/4) {#day4}

### Bingo

This is one of those problems where half the battle is working out which data structure to use.
I wrote a function [`read_draws()`](https://github.com/Selbosh/adventofcode2021/blob/main/R/day04.R#L80-L84) that reads in the first line of the file to get the drawn numbers, then separately reads in the remainder of the file to get the bingo cards stacked as a data frame.
Later we take advantage of the fact that the bingo cards are square to split the data frame into a list of matrices.

```r
read_draws <- function(file) {
  draws <- scan(file, sep = ',', nlines = 1, quiet = TRUE)
  cards <- read.table(file, skip = 1)
  list(draws = draws, cards = cards)
}
```

As numbers are called out, I replace them in the dataset with `NA`s.
Then the helper [`score_card()`](https://github.com/Selbosh/adventofcode2021/blob/main/R/day04.R#L86-L91) counts the number of `NA`s in each row and column.
If there are not enough, we return zero, else we calculate the score.

```r
score_card <- function(mat, draw) {
  marked <- is.na(mat)
  if (all(c(rowMeans(marked), colMeans(marked)) != 1))
    return(0)
  sum(mat, na.rm = TRUE) * draw
}
```

Then we put it all together, looping through the draws, replacing numbers with `NA` and halting as soon as someone wins.
Function [`play_bingo()`](https://github.com/Selbosh/adventofcode2021/blob/main/R/day04.R#L98-L111) is defined as follows, using just base R commands:

```r
play_bingo <- function(draws, cards) {
  size <- ncol(cards)
  ncards <- nrow(cards) / size
  ids <- rep(1:ncards, each = size)

  for (d in draws) {
    cards[cards == d] <- NA
    score <- sapply(split(cards, ids), score_card, draw = d)
    if (any(score > 0))
      return(score[score > 0])
  }
}
```

### Last caller

Part two is very similar, but we throw away each winning bingo card as we go to avoid redundant computation, eventually returning the score when there is only one left.
Here is function [`play_bingo2()`](https://github.com/Selbosh/adventofcode2021/blob/main/R/day04.R#L115-L131), which uses the same two utility functions:

```r
play_bingo2 <- function(draws, cards) {
  size <- ncol(cards)

  for (d in draws) {
    ncards <- nrow(cards) / size
    ids <- rep(1:ncards, each = size)
    cards[cards == d] <- NA
    score <- sapply(split(cards, ids), score_card, draw = d)
    if (any(score > 0)) {
      if (ncards == 1)
        return(score[score > 0])
      cards <- cards[ids %in% which(score == 0), ]
    }
  }
}
```

Further optimizations are possible.
For example: as written, we calculate every intermediate winner's score, but we only really need to do it for the first (part 1) and last (part 2) winners.

Also, we could draw more than one number at a time, as we know that nobody's going to win until at least the fifth draw (for 5&times;5 cards) and from there, increment according to the minimum number of unmarked numbers on any row or column.

I didn't bother implementing either of these, as it already runs quickly enough.

## Day 5 - [Hydrothermal Venture](https://adventofcode.com/2021/day/5) {#day5}

For a while I tried to think about clever mathematical ways to solve the system of inequalities, but this gets complicated when working on a grid, and where some segments are collinear.
In the end it worked out quicker to what seems like a 'brute force' approach:
generate all the points on the line segments and then simply count how many times they appear.

This is a problem that really lends itself to use of **tidyr** functions like [`separate()`](https://tidyr.tidyverse.org/reference/separate.html) and [`unnest()`](https://tidyr.tidyverse.org/reference/nest.html), so naturally I made life harder for myself by doing it in base R, instead.

First, read in the coordinates as a data frame with four columns, `x1`, `y1`, `x2` and `y2`.
The _nice_ way to do this is with `tidyr::separate()` but `strsplit()` works just fine too.
Here is my parsing function, [`read_segments()`](https://github.com/Selbosh/adventofcode2021/blob/main/R/day05.R#L77-L82):

```r
read_segments <- function(x) {
  lines <- do.call(rbind, strsplit(readLines(x), '( -> |,)'))
  storage.mode(lines) <- 'numeric'
  colnames(lines) <- c('x1', 'y1', 'x2', 'y2')
  as.data.frame(lines)
}
```

This is one of the few puzzles where the solution to part two is essentially contained in part one.
Depending on how you implement your home-rolled `unnest`-like function, it could just be a case of filtering out the diagonal lines in part one.
I make liberal use of `mapply` for looping over two vectors at once.

In the penultimate line, we take advantage of vector broadcasting, which handles all the horizontal and vertical lines where you have multiple coordinates on one axis paired with a single coordinate on the other.
For the diagonal lines, there is a 1:1 relationship so the coordinates just bind together in pairs.
Finally, we work out how to count the rows, without using `dplyr::count()`.
If you convert to a data frame, then `table()` does this for you.

```r
count_intersections <- function(lines, part2 = FALSE) {
  if (!part2)
    lines <- subset(lines, x1 == x2 | y1 == y2)
  x <- mapply(seq, lines$x1, lines$x2)
  y <- mapply(seq, lines$y1, lines$y2)
  xy <- do.call(rbind, mapply(cbind, x, y))
  sum(table(as.data.frame(xy)) > 1)
}
```

I'm fairly pleased to get the main solution down to [essentially four lines of code](https://github.com/Selbosh/adventofcode2021/blob/main/R/day05.R#L89-L96), though I'm certain that there are more computationally efficient ways of tackling this problem---if you value computer time more than your own time.

For the tidyverse approach, see [David Robinson's solution](https://twitter.com/drob/status/1467361848525787138).

## Day 6 - [Lanternfish](https://adventofcode.com/2021/day/6) {#day6}

In this problem, we have many fish with internal timers.
As the instructions suggest, we will have exponential growth, so it's not a good idea to keep track of each individual fish as you'll soon run out of memory.
On the other hand, there are only nine possible states for any given fish to be in: the number of days until they next reproduce.
So we can store a vector that simply tallies the number of fish in each state.

On each day, we can shuffle the fish along the vector, decreasing the number of days for each group of fish by 1, and adding new cohorts of fish at day 6, to represent parent fish resetting their timers, and at day 8 to represent the newly hatched lanternfish.
My short function [`lanternfish()`](https://github.com/Selbosh/adventofcode2021/blob/main/R/day06.R#L72-L77):

```r
lanternfish <- function(x, days = 80) {
  fish <- as.double(table(factor(x, levels = 0:8)))
  for (i in 1:days)
    fish <- c(fish[2:7], fish[8] + fish[1], fish[9], fish[1])
  sum(fish)
}
```

Because R indexes from 1 rather than 0, the element `fish[1]` represents the number of fish with 0 days left, `fish[2]` represents the number with 1 day left, and so on.
If you find this confusing, you can index from zero instead, thanks to the new [**index0** package](https://github.com/Selbosh/index0):

```r
lanternfish0 <- function(x, days = 80) {
  fish <- as.double(table(factor(x, levels = 0:8)))
  for (i in 1:days) {
    fish <- index0::index_from_0(fish)
    fish <- c(fish[1:6], fish[7] + fish[0], fish[8], fish[0])
   }
  sum(fish)
}
```

There is a slightly different way to perform the updates.
[David Robinson suggested](https://twitter.com/drob/status/1467727330663534594) an approach based on linear algebra.
Here we apply the same procedure as above, but via matrix multiplication.
It takes about the same time to run.

```r
lanternfish <- function(x, days = 80) {
  fish <- table(factor(x, levels = 0:8))
  mat <- matrix(0, 9, 9)
  mat[cbind(2:9, 1:8)] <- 1 # decrease timer for fish w/ 1-8 days left
  mat[1, c(7, 9)] <- 1      # add 'new' fish with 6 & 8 days left
  for (i in 1:days)
    fish <- fish %*% mat
  sum(fish)
}
```

Day 6 is another puzzle where the solutions for parts one and two are essentially the same.
The only thing to be careful of on part two is that you don't run into integer overflow.
If you do, make sure the numbers you're adding together are of type `double`.

## Day 7 - [The Treachery of Whales](https://adventofcode.com/2021/day/7) {#day7}

### Median

While it's possible to brute-force this puzzle by simply calculating the fuel requirement at every single point (within the range of the inputs), you can do it about 200&times; faster by treating it as an optimization problem.

The total fuel required for any potential position is
```r
x <- scan('input.txt', sep = ',')
f <- function(pos) sum(abs(x - pos))
```
where `x` are the initial locations of the crabs.
Then run it through `optimize()`, and round to the nearest integer position:

```r
sol <- optimize(f, range(x))$minimum
f(round(sol))
```

However, there is an even faster analytical solution!

```r
sol <- median(x)
```

Thanks to [Claire Little](https://twitter.com/claire_little1) for pointing this out.

### Mean

Part two just has a slightly different function to optimize.
Using the formula for the sum of an [arithmetic progression](https://en.wikipedia.org/wiki/Arithmetic_progression):
```r
f2 <- function(pos) {
  n <- abs(x - pos)
  sum(n / 2 * (1 + n))
}
```

Then we can simply minimize this function as before.

```r
sol <- optimize(f2, range(x))$minimum
f2(round(sol))
```

However, there's a shortcut for this part as well!
Calculate the mean of the initial positions, and work out which of the two nearest integers gives the minimum result:

```r
min(
  f2(floor(mean(x))),
  f2(ceiling(mean(x)))
)
```

Thanks to [Jonatan Pallesen](https://twitter.com/jonatanpallesen/status/1468165025575624704).
This is about 5 times faster than my optimizer.

And here is what the functions look like for my input dataset:

```{r day7, echo = FALSE, fig.width = 6, fig.height = 3, dev = 'png', dev.args = list(png = list(type = 'cairo'))}
x <- scan(adventofcode2021::input_file(7), sep = ',', quiet = TRUE)
f1 <- function(pos) sum(abs(x - pos))
f2 <- function(pos) {
  n <- abs(x - pos)
  sum(n / 2 * (1 + n))
} 
day7 <- data.frame(position = min(x):max(x),
                   fuel1 = sapply(min(x):max(x), f1),
                   fuel2 = sapply(min(x):max(x), f2))
library(ggplot2)
theme_set(theme_classic())
day7part1gg <- ggplot(day7) +
  aes(position, fuel1) +
  geom_line() +
  geom_vline(xintercept = median(x), linetype = 'dashed') +
  labs(x = 'x', y = 'fuel', title = 'Part 1') +
  geom_rug(data = data.frame(position = x, fuel1 = mean(day7$fuel1)),
           sides = 'b', outside = TRUE,
           alpha = .1, colour = 'tomato2') +
  scale_y_continuous(breaks = NULL) +
  scale_x_continuous(breaks = range(x)) +
  coord_cartesian(clip = "off")
day7part2gg <- ggplot(day7) +
  aes(position, fuel2) +
  geom_line() +
  geom_vline(xintercept = mean(x), linetype = 'dashed') +
  labs(x = 'x', y = NULL, title = 'Part 2') +
  geom_rug(data = data.frame(position = x, fuel2 = mean(day7$fuel2)),
           sides = 'b', outside = TRUE,
           alpha = .1, colour = 'tomato2') +
  scale_y_continuous(breaks = NULL) +
  scale_x_continuous(breaks = range(x)) +
  coord_cartesian(clip = "off")
gridExtra::grid.arrange(day7part1gg, day7part2gg, ncol = 2)
```

## Day 8 - [Seven Segment Search](https://adventofcode.com/2021/day/8) {#day8}

### Unique digits

Read in the data and then the first part is just a one-liner:

```r
input <- do.call(rbind, strsplit(readLines(input_file(8)), '[^a-z]+'))

count_unique <- function(x) {
  sum(nchar(x[, -(1:10)]) %in% c(2, 3, 4, 7))
}
```

### Segment matching

I _really_ wanted to solve part two using graph theory, by representing the puzzle as a maximum bipartite matching problem.
However, I couldn't quite get this to work.
My final solution is instead just a lot of leg work.

Essentially you solve the problem by hand and then encode the process programmatically.
Recognize that some digits have segments in common, or not in common, and use this to eliminate the possibilities.
I stored the solutions in a named vector, which I was able to use to look up the digits found so far.

The function `setdiff()` comes in useful.

```r
contains <- function(strings, letters) {
  vapply(strsplit(strings, ''),
         function(s) all(strsplit(letters, '')[[1]] %in% s),
         logical(1))
}

output_value <- function(vec) {
  segments <- c('abcefg', 'cf', 'acdeg', 'acdfg', 'bcdf',
                'abdfg', 'abdefg', 'acf', 'abcdefg', 'abcdfg')
  nchars <- setNames(nchar(segments), 0:9)

  # Sort the strings
  vec <- sapply(strsplit(vec, ''), function(d) paste(sort(d), collapse = ''))
  sgn <- head(vec, 10)
  out <- tail(vec, 4)

  # Store the known values
  digits <- setNames(character(10), 0:9)
  unique <- c('1', '4', '7', '8')
  digits[unique] <- sgn[match(nchars[unique], nchar(sgn))]

  # Remaining digits have 5 or 6 segments:
  sgn <- setdiff(sgn, digits)
  digits['3'] <- sgn[nchar(sgn) == 5 & contains(sgn, digits['1'])]
  digits['6'] <- sgn[nchar(sgn) == 6 & !contains(sgn, digits['1'])]
  sgn <- setdiff(sgn, digits)
  digits['0'] <- sgn[nchar(sgn) == 6 & !contains(sgn, digits['4'])]
  sgn <- setdiff(sgn, digits)
  digits['9'] <- sgn[nchar(sgn) == 6]
  sgn <- setdiff(sgn, digits)
  digits['2'] <- sgn[
    contains(sgn, do.call(setdiff,
                          unname(strsplit(digits[c('8', '6')], ''))))
  ]
  digits['5'] <- setdiff(sgn, digits)

  # Combine four output digits:
  as.numeric(paste(match(out, digits) - 1, collapse = ''))
}
```

## Day 9 - [Smoke Basin](https://adventofcode.com/2021/day/9) {#day9}

### Lowest points

You can find all the lowest points with a one-liner:

```r
lowest <- function(h) {
  h < cbind(h, Inf)[, -1] &          # right
    h < rbind(h, Inf)[-1, ] &        # down
    h < cbind(Inf, h[, -ncol(h)]) &  # left
    h < rbind(Inf, h[-nrow(h), ])    # up
}
```

Then do `sum(h[lowest(h)])` to get the result, where `h` is a numeric matrix of the input data.

### Basins

The second part is harder and doesn't immediately lead from the first.
Initially I thought of replacing each lowest point with `Inf`, then finding the new lowest points and repeating the process until all the basins are found.
However, the basins are simply all points where the height is `< 9`, so you can find the basins in a single step.

The tricky part is labelling them separately, so you can count up their respective sizes.

The boring way of doing this is just to loop over the indices and label the points that neighbour already-labelled ones (starting with the lowest points as the initial labels), doing several passes until everything (except the 9s) is labelled.

```r
basins <- function(h) {
  l <- lowest(h)
  h[] <- ifelse(h < 9, NA, Inf)
  h[l] <- 1:sum(l)
  while (anyNA(h)) {
    for (i in 1:nrow(h)) for (j in 1:ncol(h)) {
      if (is.na(h[i, j])) {
        nbrs <- h[cbind(c(max(i - 1, 1), min(i + 1, nrow(h)), i, i),
                        c(j, j, max(j - 1, 1), min(j + 1, ncol(h))))]
        if (any(is.finite(nbrs)))
          h[i, j] <- nbrs[is.finite(nbrs)][1]
      }
    }
  }
  sizes <- table(h[is.finite(h)])
  head(sort(sizes, decreasing = TRUE), 3)
}
```

To vectorize this in the same way as part one, we define a new binary (infix) operator `%c%`, analogous to `dplyr::coalesce()`.
What this does is replace an `NA` value (a basin not yet assigned a label) with its finite neighbour, while leaving `Inf`s (marking basin edges) alone.

```r
"%c%" <- function(x, y) {
  ifelse(is.infinite(x), x,
         ifelse(!is.na(x), x,
                ifelse(!is.infinite(y), y, x)))
}
```

Then the new function for part two is as follows.
It is five times faster to run than the nested loop above.

```r
basins2 <- function(h) {
  l <- lowest(h)
  h[] <- ifelse(h < 9, NA, Inf)
  h[l] <- 1:sum(l)
  while(anyNA(h)) {
    h <- h %c%
      cbind(h, NA)[, -1] %c%        # right
      rbind(h, NA)[-1, ] %c%        # down
      cbind(NA, h[, -ncol(h)]) %c%  # left
      rbind(NA, h[-nrow(h), ])      # up
  }
  sizes <- table(h[is.finite(h)])
  head(sort(sizes, decreasing = TRUE), 3)
}
```

You can also [formulate this as an image analysis problem](https://twitter.com/rappa753/status/1468876602016735233), effectively treating each basin as an area of similar colour to select, or you can [treat it as a network theory problem and apply the **igraph** package](https://twitter.com/babeheim/status/1468898580408811525) to find graph components.

## Day 10 - [Syntax Scoring](https://adventofcode.com/2021/day/10) {#day10}

### Corrupt characters

Whilst it's probably possible to do the first part with some very fancy [recursive regular expressions](https://www.php.net/manual/en/regexp.reference.recursive.php), I don't know how to use them.

Instead, my method of finding unmatched brackets is simply to search for empty pairs of brackets and successively strip them from the string.
Keep doing this until the strings stop changing.
Then, get the first closing bracket (if any), using `regmatches()`.
These are the illegal characters.

My function [`syntax_score()`](https://github.com/Selbosh/adventofcode2021/blob/main/R/day10.R#L122-L128) is implemented as follows:

```r
lines <- readLines('input.txt')
old <- ''
while (!identical(old, lines -> old))
  lines <- gsub(r'(\(\)|<>|\{\}|\[\])', '', lines)
illegals <- regmatches(lines, regexpr(r'(\)|>|\}|\])', lines))
```
The syntax score is calculated using a named vector as a lookup table.
```r
illegal_score <- c(')' = 3, ']' = 57, '}' = 1197, '>' = 25137)
sum(illegal_score[illegals])
```

### Autocomplete

Part two starts the same, but instead of extracting the illegal characters we just throw away those lines that contain them.

```r
illegals <- grep(r'(\)|>|\}|\])', lines)
chars <- strsplit(lines[-illegals], '')
```
From here, we can calculate the scores using a `Reduce` operation (from right to left) with another lookup table.
The final answer is the median score.
```r
complete_score <- c('(' = 1, '[' = 2, '{' = 3, '<' = 4)
scores <- sapply(chars, Reduce, init = 0, right = TRUE,
                 f = \(c, s) 5 * s + complete_score[c])
median(scores)
```
The function [`autocomplete()`](https://github.com/Selbosh/adventofcode2021/blob/main/R/day10.R#L132-L141) wraps it all together.

## Day 11 - [Dumbo Octopus](https://adventofcode.com/2021/day/11) {#day11}

### Convoluted octopuses

The process of updating the energy levels can be described using a [_convolution matrix_](https://en.wikipedia.org/wiki/Kernel_(image_processing)).
It's easy---[like on Day 11 last year](https://selbydavid.com/2020/12/06/advent-2020/#day11)---to use a ready-made solution from an image analysis package for this, namely `OpenImageR::convolution()`.

The convolution matrix, or _kernel_ is
\[\begin{bmatrix}1  &  1  &  1 \\1  &   0  &  1 \\1  &  1  &  1\end{bmatrix},\]
to be applied on an indicator matrix of 'flashing' octopuses, and then added to the result.
In R,
```r
kernel <- matrix(1:9 != 5, 3, 3)
```

So we define a function, `step1()`, that applies a single step of the energy level updating process.
Since each octopus can only flash once in a given step, we keep track of those that have already flashed, as well as those currently flashing.
A short `while()` loop repeats until no more octopuses flash.

```r
step1 <- function(x) {
  x <- x + 1
  flashing <- flashed <- x == 10
  while (any(flashing)) {
    x <- x + OpenImageR::convolution(flashing, kernel)
    flashing <- x > 9 & !flashed
    flashed <- flashing | flashed
  }
  x[x > 9] <- 0
  x
}
```

However, there is a base R alternative to `OpenImageR::convolution()` that you can substitute in, for negligible speed penalty (despite that package being written in C).

```r
add_neighbours <- function(x) {
  I <- nrow(x)
  J <- ncol(x)
  cbind(x[, -1], 0) +  # Right
  rbind(x[-1, ], 0) +  # Down
  cbind(0, x[, -J]) +  # Left
  rbind(0, x[-I, ]) +  # Up
  rbind(cbind(x[-1, -1], 0), 0) + # SE
  rbind(0, cbind(x[-I, -1], 0)) + # NE
  rbind(cbind(0, x[-1, -J]), 0) + # SW
  rbind(0, cbind(0, x[-I, -J]))   # NW
}
```

### Counting flashes

We then solve both parts one and two with a single function that repeatedly applies the updating step and counts the flashes (number of zeros) each time.

By default, it simply completes `iter` steps and then returns cumulative total number of flashes.

In `part2` mode, the function will terminate early if it encounters a step where all the octopuses are flashing and return the iteration number, otherwise it will generate a warning.

```r
count_flashes <- function(x, iter = 100, part2 = FALSE) {
  count <- 0
  for (i in 1:iter) {
    x <- step1(x)
    nflashes <- sum(x == 0)
    if (part2 & nflashes == prod(dim(x)))
      return(i)
    count <- count + nflashes
  }
  if (part2)
    warning('No synchronization detected in ', iter, ' steps')
  count
}
```

The whole thing runs in about 20 milliseconds on my input dataset, or 60 milliseconds using the base R convolution function.

## Day 12 - [Passage Pathing](Passage Pathing) {#day12}

### Counting paths

Today's puzzle is very obviously a graph theory problem, so it would seem intuitive to break out **igraph** for this.
However, the functions in that package are mainly designed for finding the _shortest_ paths from A to B, not a complete list of all possible paths.

Therefore, it's easier (as far as I can tell) to solve this with your own data structure and a recursive function or two.

[Antoine Fabri](https://twitter.com/antoine_fabri/status/1469989831522541574) suggested a rather neat solution of describing the graph with a named list, where the names of the elements describe a node, and the vector contained therein list the other nodes reachable from that point.

We can read in the data as follows:

```r
read_edges <- function(file) {
  edges <- read.table(file, sep = '-', col.names = c('from', 'to'))
  edges <- rbind(edges, setNames(edges, rev(colnames(edges))))
  edges <- edges[edges$to != 'start', ]
  split(edges$to, edges$from)
}
```

Then we solve part one with a recursive function that, starting from `start`, traverses a path all the way to `end` and increments a counter.
To avoid revisiting small caves, we discard any edges to vertices with lowercase names, with the aid of `tolower()`.

```r
count_paths <- function(edgelist, node = 'start') {
  if (node == 'end')
    return(1)
  if (!length(edgelist[[node]]))
    return(0)
  if (node == tolower(node))
    edgelist <- lapply(edgelist, \(v) v[node != v])
  sum(vapply(edgelist[[node]], count_paths, e = edgelist,
             FUN.VALUE = numeric(1)))
}
```

### Revisiting caves

Part two is just a slight adaptation.
This time we can't simply delete small caves as soon as we visit them, so we need to keep track of those visited so far, while keeping the graph intact.
Once we find ourselves on a node that we have already visited, it means we are visiting it for the second time, so at this point we can delete all the visited nodes and switch over to the function from part one.

```r
count_paths2 <- function(edgelist, node = 'start', visited = NULL) {
  if (node == 'end')
    return(1)
  if (node == tolower(node)) {
    if (node %in% visited) {
      edgelist <- lapply(edgelist, \(v) v[!v %in% visited])
      return(count_paths(edgelist, node))
    }
    visited <- union(visited, node)
  }
  if (!length(edgelist[[node]]))
    return(0)
  sum(vapply(edgelist[[node]], count_paths2, e = edgelist, visited,
             FUN.VALUE = numeric(1)))
}
```

R is quite slow at recursion, so this second function takes about 6 seconds to run on my full input data.
It may be possible to optimize this further, but that might require the use of **igraph**, which is written in C, rather than a base R solution.

## Day 13 - [Transparent Origami](https://adventofcode.com/2021/day/13) {#day13}

### Folding paper

The hardest bit about this puzzle is reading in the data, since we have two different data structures in the same file.

The neatest way of doing this is to find the separator (a blank line) in the middle of the file, then read through the parts of the file before and after this separately.
Since there's only one file to read, you could of course just inspect it in a text editor and find out this line number by hand.
But here's a programmatic method.

```r
n <- which(readLines('input.txt') == '')
paper <- read.table('input.txt', sep = ',', nrows = n - 1,
                    col.names = c('x', 'y'))
folds <- read.table('input.txt', sep = '=', skip = n)
```

Next, we need a function that'll fold the paper in half once.
My initial solution, which works perfectly well, is to convert the `x` and `y` coordinates to a matrix representation, then simply update the matrix using subset operations.

```r
mat <- with(paper, matrix(0, max(x) + 1, max(y) + 1))
mat[data.matrix(paper) + 1] <- 1
```

Watch out for R's [one-based indexing](/2021/12/06/indexing/).
We want to fold along the coordinate `f`, which is at index `f + 1`, and so the indices either side are `f` and `f + 2`.

```r
fold_once <- function(x, dir, f) {
  if (dir == 'fold along y') {
    x[, 1:f] | x[, ncol(x):(f + 2)]
  } else {
    x[1:f, ] | x[nrow(x):(f + 2), ]
  }
}
```

At this point you might notice that the paper is always being folded exactly in half every time.
So the value after the `=` in the input is actually redundant; it's always the middle of the matrix.
For instance, in the example data, the matrix has dimension 15&times;11 and we fold along $y=7$ (i.e. index 8), which is half of 15.
The next instruction $x=5$ (index 6) is half of the other dimension.

From here, loop over the directions and then plot the final matrix using `image()`.

### Tidy origami

Why do they give us the values to `fold along` if it's always the middle value?
Because it's not actually very efficient to store these sparse coordinates in a matrix.
Instead, recognize, as pointed out by [Riinu Pius](https://twitter.com/_Riinu_/status/1470320577340755972) and [Antoine Fabri](https://twitter.com/antoine_fabri/status/1470294716000485376), that you can just keep the coordinates in a long format and fold them as follows:

```r
fold_once <- function(x, d, f) {
  x[, d] <- ifelse(x[, d] >= f, 2 * f - x[, d], x[, d])
  x[!duplicated(x), ]
}
```

Then loop over all the instructions with:

```r
fold_paper <- function(x, folds, n = nrow(folds)) {
  for (i in 1:n)
    x <- fold_once(x, folds[i, 1], folds[i, 2])
  x
}
```

And finally you can plot the result using **ggplot2**.

```{r day13, fig.width = 6, fig.height = 2}
library(adventofcode2021)
input <- read_origami(input_file(13))
input$n <- nrow(input[[2]])
folded <- do.call(fold_paper, input)

library(ggplot2)
ggplot(folded) +
  aes(x, y) +
  geom_tile(fill = '#f5c966') +
  coord_fixed() +
  scale_y_reverse() +
  theme_void()
```

## Day 14 - [Extended Polymerization](https://adventofcode.com/2021/day/14) {#day14}

### Inserting letters

The initial approach to this problem is to perform the insertions and build up an ever-lengthening string.
There might be a way to do this using regular expressions, but I used an approach based on table joins.

First, read in the data:

```r
template <- strsplit(readLines('input.txt', n = 1), '')[[1]]
template <- data.frame(first = head(template, -1),
                       second = tail(template, -1))
rules <- read.table('input.txt', skip = 1)
rules <- data.frame(first  = substr(rules[, 1], 1, 1),
                    second = substr(rules[, 1], 2, 2),
                    insert = rules[, 3])
```

This should give you two data frames that look something like this:

```
  first second
1     N      N
2     N      C
3     C      B

   first second insert
1      C      H      B
2      H      H      N
3      C      B      H
4      N      H      C
5      H      B      C
6      H      C      B
7      H      N      C
8      N      N      C
9      B      H      H
10     N      C      B
11     N      B      B
12     B      N      B
13     B      B      N
14     B      C      B
15     C      C      N
16     C      N      C
```

From here, the method is straightforward.
Join the `template` with the `rules` and produce a new `template` comprised of the pairs `first -> insert` and `insert -> second`.

```r
template <- with(merge(template, rules),
                 data.frame(first = c(first, insert),
                            second = c(insert, second)))
```

Repeat this process the required number of times and we will have a data frame containing all the pairs of letters.

There's a problem, though: `merge()` does not preserve row order, so we've lost track of which letters are the first and the last in the sequence.
This means counting up the letters in either column `first` or `second` is going to undercount by 1.
We could just refer back to the original input, since the first and last letters won't change.

However, there's another neat solution, which takes advantage of the fact that all the middle characters will appear in both columns, but the first and last values, if different, will appear an odd number of times.

```r
counts <- (table(unlist(template)) + 1) %/% 2
```

From here, the difference between the minimum and maximum counts is given by:

```r
diff(range(counts))
```

### Scalable insertion

The above approach works for part one, but once we increase the iteration count to 40, the data frame or string representation is far too long to store in memory.
It simply doesn't scale.

Rather than keep the entire sequence and then count up the letters at the end, we can instead store pair frequencies.
To do this, we can call `dplyr::count()` or use a base R approach.

Add a frequency column to the original data frame:
```r
template <- data.frame(first  = head(template, -1),
                       second = tail(template, -1),
                       n = 1)
```
or if you want to aggregate on initialization, you could use:
```r
template <- as.data.frame(table(template), stringsAsFactors = FALSE)
```

Then after each round of insertions, `aggregate()` the counts:
```r
data <- with(merge(template, rules),
             data.frame(first = c(first, insert),
                        second = c(insert, second)))
template <- aggregate(n ~ first + second, data, sum)
```

Finally, the total letter counts are given by:

```r
tab <- xtabs(n ~ first + second, template)
(rowSums(tab) + colSums(tab) + 1) %/% 2
```

## Day 15 - [Chiton](https://adventofcode.com/2021/day/15) {#day15}

The answer is [Dijkstra's algorithm](https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm), whether you're aware of it or not.
That is, the problem is a weighted, directed graph and you've been tasked with finding the shortest path between two points.

You could write your own implementation in R, but it'll be slow, so I am going to use the off-the-shelf solution in the **igraph** package.
The hardest task is converting from a matrix into a 2-dimensional lattice graph.

```r
lattice_from_matrix <- function(m) {
  edges <- data.frame(node1 = c(which(row(m) < nrow(m)),
                                which(col(m) < ncol(m))),
                      node2 = c(which(row(m) > 1),
                                which(col(m) > 1)))
  edges <- rbind(edges, setNames(edges, c('node2', 'node1')))
  edges$weight <- m[edges$node2]
  igraph::graph_from_data_frame(edges)
}
```

Then, just let the algorithm do the work.

```r
lowest_total_risk <- function(g) {
  as.vector(igraph::distances(g, v = 1, to = igraph::vcount(g),
                              mode = 'out', algorithm = 'dijkstra'))
}
```

For part two, we need to tile the input and then rerun the code again.

```r
tile_matrix <- function(m, times = 5) {
  m <- Reduce(cbind, lapply(seq_len(times) - 1, '+', m))
  m <- Reduce(rbind, lapply(seq_len(times) - 1, '+', m))
  m[m > 9] <- m[m > 9] %% 10 + 1
  m
}
```

The final search takes about 0.8 seconds, which is faster than any base-R Dijkstra loop would be.

## Day 16 - [Packet Decoder](https://adventofcode.com/2021/day/15) {#day16}

### Versions

This puzzle takes a long time to read.
The solution involves recursive functions that read through the packets and subpackets in turn.
My approach takes the bits and throws them away as they are processed, until there are none left (or, for sub-packets, if we have finished parsing the requisite number).

We are working between bases here, so you want a couple of utility functions to convert to and from integers.

```r
to_bits <- function(x) {
  ints <- strtoi(strsplit(x, '')[[1]], 16)
  bits <- sapply(ints, \(n) tail(rev(as.numeric(intToBits(n))), 4))
  as.vector(bits)
}
```

Unfortunately, `strtoi` is not your friend for the binary-to-integer conversion, because it will almost surely silently introduce integer overflow on your main input data.
Here's one I prepared earlier that uses floating point numbers so it won't overflow.

```r
binary_to_int <- function(x) {
  sum(x * 2 ^ rev(seq_along(x) - 1))
}
```

Now here's the main recursion.

```r
packet_versions <- function(bits, depth = 0, rem_subs = Inf, eval = FALSE) {
  acc <- 0
  while (any(bits > 0) & rem_subs > 0) {
    rem_subs <- rem_subs - 1
    version <- binary_to_int(bits[1:3])
    acc <- acc + version
    type <- binary_to_int(bits[4:6])
    bits <- tail(bits, -6)
    if (type == 4) { # literal value
      n_groups <- which.min(bits[seq(1, length(bits), 5)])
      sub <- head(bits, n_groups * 5)
      sub <- sub[(seq_along(sub) - 1) %% 5 > 0]
      # literal_value <- binary_to_int(sub)
      bits <- tail(bits, -n_groups * 5)
      next
    }
    # Operator mode:
    lentype <- bits[1]
    bits <- bits[-1]
    if (lentype == 0) {
      sub_length <- binary_to_int(bits[1:15])
      bits <- tail(bits, -15)
      sub  <- head(bits, sub_length)
      acc <- acc + packet_versions(sub, depth + 1)[['acc']]
      bits <- tail(bits, -sub_length)
    } else {
      n_subs <- binary_to_int(bits[1:11])
      bits <- tail(bits, -11)
      sub_result <- packet_versions(bits, depth + 1, n_subs)
      acc <- acc + sub_result[['acc']]
      bits <- tail(bits, sub_result[['length']])
    }
  }
  if (depth)
    acc <- c(acc = acc, length = length(bits))
  acc
}
```

Be careful with length type ID 1, because you need to save the number of bits you've parsed and then send this number back to the main function.
Similarly, it's a good idea to keep track of how deep in the recursion you are at any given time.

### Evaluation

The second part is tricky. I decided to adapt my function from part one to generate syntactically-valid R expressions, so that final step is simply evaluating machine-generated code.

```r
packet_decode <- function(bits, depth = 0, rem_subs = Inf) {
  acc <- NULL
  while (any(bits > 0) & rem_subs > 0) {
    rem_subs <- rem_subs - 1
    type <- binary_to_int(bits[4:6])
    bits <- tail(bits, -6)
    op <- c('sum', 'prod', 'min', 'max', 'c', '`>`', '`<`', '`==`')[type + 1]
    if (type == 4) { # literal value
      n_groups <- which.min(bits[seq(1, length(bits), 5)])
      sub <- head(bits, n_groups * 5)
      sub <- sub[(seq_along(sub) - 1) %% 5 > 0]
      literal_value <- binary_to_int(sub)
      acc$expr <- c(acc$expr, literal_value)
      bits <- tail(bits, -n_groups * 5)
      next
    }
    # Operator mode:
    lentype <- bits[1]
    bits <- bits[-1]
    if (lentype == 0) {
      sub_length <- binary_to_int(bits[1:15])
      bits <- tail(bits, -15)
      sub  <- head(bits, sub_length)
      sub_result <- packet_decode(sub, depth + 1)
      subexpr <- c(op, sub_result[['expr']])
      acc$expr <- c(acc$expr, list(subexpr))
      bits <- tail(bits, -sub_length)
    } else {
      n_subs <- binary_to_int(bits[1:11])
      bits <- tail(bits, -11)
      sub_result <- packet_decode(bits, depth + 1, n_subs)
      subexpr <- c(op, sub_result[['expr']])
      acc$expr <- c(acc$expr, list(subexpr))
      bits <- tail(bits, sub_result[['length']])
    }
  }
  if (depth)
    acc$length <- length(bits)
  acc
}
```

The structure of the function is essentially the same, and the result is a deeply nested list representing the hierarchical tree of expressions.
From here we want to convert it from a tree into some code, and the function `combine_expr` does this for us:

```r
combine_expr <- function(node, eval = FALSE) {
  if (length(node) == 1)
    return(node)
  expr <- sprintf('%s(%s)', node[1], paste(node[-1], collapse = ', '))
  if (!eval)
    return(expr)
  eval(str2expression(expr))
}
```

However, it will only work on leaves of the tree.
How can we correctly operate on the entire tree and turn it into a single expression?
That requires more recursion.
Annoyingly, the function `rapply` doesn't quite do what's needed, so we introduce yet another function, called `packet_parse`:

```r
packet_parse <- function(tree, eval = FALSE) {
  if (!is.list(tree))
    return(combine_expr(tree, eval))
  expr <- packet_parse(unlist(lapply(tree, packet_parse, eval), use.names = F))
  if (eval & is.character(expr))
    return(eval(parse(text = expr)))
  expr
}
```

You can choose to view the expression, or evaluate it.
Here is my final expression generated by `packet_parse`:

```r
sum(prod(425542, `<`(247, 247)), sum(121, 21236), prod(`>`(sum(11, 12, 11), sum(7, 10, 7)), 32566), prod(`<`(sum(8, 7, 15), sum(6, 11, 10)), 4507180), min(prod(prod(sum(prod(max(prod(min(prod(min(prod(max(sum(sum(max(sum(sum(min(prod(prod(130)))))))))))))))))))), 139930778832, prod(`>`(52, 667118), 10), 602147, max(62199), prod(14849899, `<`(11716, 26963)), prod(4083, `>`(135, 135)), prod(135, 217, 224), 73, prod(sum(13, 4, 9), sum(12, 15, 7), sum(13, 10, 9)), min(194), prod(182, 197, 136, 2, 242), prod(226, 142, 34, 124), max(4025, 186042), min(30059, 126119002), min(9, 260, 162), prod(`<`(4, 4), 28699), prod(1945, `==`(1714, 1714)), prod(7, `<`(1545, 108)), sum(12), prod(200, `>`(31050, 655605)), 3154, prod(3, `<`(64896, 116)), 3055, prod(13), min(48082, 226938, 1175, 68077774919), sum(66, 15, 181, 1380642642, 11831587), prod(241, 59), prod(150, `>`(2742, 113)), 37007908601, max(52444, 11, 13008816, 2935), 20723, 8, prod(5, `>`(6241732, 759708)), sum(prod(15, 7, 4), prod(14, 2, 12), prod(13, 6, 6)), sum(2877, 229333, 655820, 1020971), sum(39581, 2, 14), max(982557, 44, 31), 68, prod(`==`(11530, 3492), 41177), prod(`==`(236, 918711093), 3937), max(903466, 228, 6, 25989131, 4028), 229, min(299875, 10969849, 11481, 2281, 13), prod(55300721, `>`(63, 63)), prod(244, `>`(sum(7, 13, 7), sum(12, 5, 14))), prod(4494263, `==`(sum(4, 15, 4), sum(3, 3, 14))), prod(`<`(45, 3307915), 58514), prod(3596530693, `<`(sum(3, 12, 4), sum(9, 11, 2))))
```

Which yields the answer `184487454837` when evaluated.

## Day 17 - [Trick Shot](https://adventofcode.com/2021/day/15) {#day17}

A little bit of secondary-school mathematics can help your search for the highest trajectory, and for the total number of trajectories.
Key is that the $x$ and $y$ movements are essentially independent.

Remember the formula
$$
v = u + at
$$
gives velocity $v$ at time $t$ for initial velocity $u$ and average acceleration $a$.
How long does it take to reach the peak?
Rearranging and plugging in values:
$$t = \frac{v - u} a = \frac{0 - u}{-1} = u,$$
which tells us that the probe launched with vertical velocity $u$ _always_ reaches its peak at time $t = u$ (except if pointed downwards: then its highest point is the launch position 0).
At that time, the sum of the arithmetic progression is
$$
x = \frac{t}2 \Bigl(2u + (t - 1)a\Bigr) = u^2 + \frac{u - u^2}2 = \frac{u(u+1)}{2},
$$
for any launch velocity $u$.
Hence, the peak of the curve is always $\max\{0, u_y(u_y + 1) / 2\}$ where $u_y$ is the vertical component of $u$.

Other insights to narrow the search space:

- At time $t$, the probe is at position $$x = \frac{t}2\left(2u - 1+t)\right)$$ (using the formula above), so there is no need to loop through successive time points.

- Rearrange that formula to get the time it takes for a probe with initial velocity $u$ to reach point $x$: $$t = \frac12\left(\sqrt{4u^2+4u-8x+1} + 2u + 1\right).$$

- The maximum horizontal velocity $u_x$ is equal to the right-most edge of the target area. Any faster and the probe will immediately overshoot.

- It takes $t = u_x(u_x+1)/2$ time points for the probe to be slowed down by drag to zero horizontal velocity. This gives us a lower bound $$u_x > \frac12 \left(\sqrt{8x_1+1} - 1\right),$$ because if the initial velocity is any less than this then the probe will never reach the left edge of the target area.

In the end I didn't use all these facts, because a 'brute force' type search over several points seems to be fast enough and less fiddly to get the formulae right.
But it could certainly be faster if using these points.

```r
search_trajectories <- function(target, trick = TRUE) {
  max_x <- target['x2']
  min_x <- ceiling((sqrt(8 * target['x1'] + 1) - 1) / 2)
  max_y <- -target['y1'] - 1
  min_y <- target['y1']
  n <- 0
  for (y in max_y:min_y) {
    end_time <- time_to_reach(y, target['y1']) - y
    y_seq <- -cumsum(-y:end_time)
    if (min(y_seq) > target['y1'])
      warning('sequence too short for y = ', y)
    for (x in max_x:min_x) {
      x_seq <- cumsum(x:0)
      if (x < end_time + y) {
        x_seq <- c(x_seq, rep(x_seq[x], end_time + y - x))
      } else x_seq <- head(x_seq, end_time + y + 1)
      if (any(x_seq >= target['x1'] & x_seq <= target['x2'] &
              y_seq >= target['y1'] & y_seq <= target['y2'])) {
        if (trick)
          return(y * (y + 1) / 2)
        n <- n + 1
      }
    }
  }
  n
}
```

In principle---though I couldn't quite get it to work---we could avoid the loops by calculating the times it takes the probe's $y$-position to reach the top/bottom of the target area, using. Then plug those times into the position formula to see if the $x$-coordinate is also within the target area at those times.
If so, then it hits the target.

```r
position <- function(u, t) {
  t / 2 * (2 * u + (t - 1) * -1)
}

time_to_reach <- function(u, x) {
  round(1/2 * (sqrt(4 * u^2 + 4 * u - 8 * x) + 2 * u + 1))
}
```

## Day 18 - [Snailfish](https://adventofcode.com/2021/day/18) {#day18}

### Reduction

I decided (for better or for worse) to convert the square brackets into `list(...)` expressions, then parse the input as nested lists.

```r
read_sf <- function(input) {
    out <- lapply(input, function(string) {
        expr <- gsub('\\]', ')', gsub('\\[', 'list(', string))
        eval(parse(text = expr))
    })
    if (length(out) == 1) return(unlist(out, FALSE)) else out
}
```

Then we need to perform the reductions on our list structures.
Splitting is easy, but it's important that we split only the first node we find and then stop.
I used a counter called `done` to ensure the `rapply` loop terminates early.

```r
sf_split <- function(lst) {
    done <- FALSE
    rapply(lst, \(n) {
        if (n <= 9 | done) return(n)
        done <<- TRUE
        list(floor(n / 2), ceiling(n / 2))
    }, how = 'replace')
}
```

To explode, it's a bit trickier because you need several nodes to interact with one another.
I wrote some recursive functions to find the depth of the list, to mark the appropriate node with an `NA` and then to replace it.

I also discovered the `relist` function, which makes it possible to flatten a list, do some vector operations (handy for getting the indices just before and after our exploded pair) and then return the data to the original nested structure.

```r
depths <- function(lst, depth = 0) {
    if (!is.list(lst)) return(depth)
    unlist(sapply(lst, depths, depth + 1))
}

replaceNA <- function(lst, replacement) {
    if (all(is.na(lst))) return(replacement)
    if (!is.list(lst)) return(lst)
    lapply(lst, replaceNA, replacement)
}

sf_explode <- function(lst) {
    i <- which(depths(lst) > 4)
    L <- i[1]; R <- i[2]
    x <- unlist(lst)
    if (L > 1)
        x[L - 1] <- x[L - 1] + x[L]
    if (R < length(x))
        x[R + 1] <- x[R] + x[R + 1]
    x[c(L, R)] <- NA
    replaceNA(relist(x, lst), 0)
}
```

Then we put it all together as reduce operations.

```r
sf_reduce <- function(lst) {
    Reduce(\(a, b) list(sf_reduce1(a), b), lst) |> sf_reduce1()
}

sf_reduce1 <- function(lst) {
    if (max(depths(lst)) > 4) lst <- Recall(sf_explode(lst))
    if (any(unlist(lst) > 9)) lst <- Recall(sf_split(lst))
    lst
}
```

If you try to reduce the whole list in one operation, you'll definitely get a stack overflow with the larger input datasets.
Thus it's important to reduce from the left until it's as simple as possible, then combine it with the next line.

My code is quite slow; it might be more sensible to rewrite this as a `repeat` loop rather than doing all this recursion.

Finally, calculate the magnitude.

```r
magnitude <- function(lst) {
    if (!is.list(lst)) return(lst)
    Reduce(\(a, b) 3 * magnitude(a) + 2 * magnitude(b), lst)
}
```

### Pairs

There might be a clever way of finding the biggest pair but I use brute force:

```r
combos <- t(combn(length(input), 2))
combos <- rbind(combos, combos[, 2:1])
magnitudes <- apply(combos, 1, \(i) magnitude(sf_reduce(input[i])))
max(magnitudes)
```
