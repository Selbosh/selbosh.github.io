---
title: Advent of Code 2021
date: '2021-12-01T09:00:00+00:00'
slug: advent-2021
categories: ['R']
images: ['/img/2020/adventofcode.jpg']
---

It's that time of year again.
And not just for [Secret Santa](/2016/12/07/santa/)---it's time for the [Advent of Code](https://adventofcode.com/), a series of programming
puzzles in the lead-up to Christmas.

I'm doing the 2021 challenge in R---in the form of an open-source [R package](https://github.com/Selbosh/adventofcode2021), to demonstrate a [test-driven](https://personalpages.manchester.ac.uk/staff/david.selby/rthritis/2021-11-19-unittesting/) workflow.

<div style="text-align:center;">
  <div class="github-card" data-github="Selbosh/adventofcode2021" data-width="400" data-height="" data-theme="default" style="display:block; margin:0 auto;"></div>
</div>
<script src="//cdn.jsdelivr.net/github-cards/latest/widget.js"></script>

Each puzzle description typically comes with a few simple examples of inputs and outputs.
We can use these to define expectations for unit tests with the [**testthat**](https://testthat.r-lib.org/) package.
Once a function passes the unit tests, it should be ready to try with the main puzzle input.

Check my [**adventofcode2021**](https://github.com/Selbosh/adventofcode2021) repository on GitHub for the latest.

```r
remotes::install_github('Selbosh/adventofcode2021')
```

1. [Sonar Sweep](#day1)
2. [Dive!](#day2)
3. [Binary Diagnostic](#day3)
4. [Giant Squid](#day4)
5. [Hydrothermal Venture](#day5)
6. [Lanternfish](#day6)
7. [The Treachery of Whales](#day7)
8. [Seven Segment Search](#day8)
9. [Smoke Basin](#day9)
10. [Syntax Scoring](#day10)
11. [Dumbo Octopus](#day11)

## Day 1 - [Sonar Sweep](https://adventofcode.com/2021/day/1) {#day1}

### Increases

To count the number of times elements are increasing in a vector it's as simple as

```r
depths <- c(199, 200, 208, 210, 200, 207, 240, 269, 260, 263)
sum(diff(depths) > 0)
```

```
## [1] 7
```

for which I defined a function called [`increases`](https://github.com/Selbosh/adventofcode2021/blob/main/R/day01.R#L91-L93).

### Rolling sum

For part two, we first want to calculate the three-depth moving sum, then we count the increases as in part one.
There are plenty of solutions in external R packages for getting lagged (and leading) vectors, for instance `dplyr::lag()` and `dplyr::lead()`: 

```r
depths + dplyr::lead(depths) + dplyr::lead(depths, 2)
```

```
##  [1] 607 618 618 617 647 716 769 792  NA  NA
```

Or you could even calculate the rolling sum using a pre-made solution in **zoo** (Z's Ordered Observations, a time-series analysis package).

```r
zoo::rollsum(depths, 3)
```

```
## [1] 607 618 618 617 647 716 769 792
```

To avoid loading any external packages at this stage, I defined my own base R function called [`rolling_sum()`](https://github.com/Selbosh/adventofcode2021/blob/main/R/day01.R#L99-L101), which uses `tail` and `head` with negative lengths to omit the first and last elements of the vector:

```r
head(depths, -2) + head(tail(depths, -1), -1) + tail(depths, -2)
```

```
## [1] 607 618 618 617 647 716 769 792
```

As [David Schoch points out](https://twitter.com/schochastics/status/1466062839077027845), you can just use the `lag` argument of `diff` to make this entire puzzle into a one-liner:

```r
sapply(c(1, 3), \(lag) sum(diff(depths, lag) > 0))
```

```
## [1] 7 5
```

## Day 2 - [Dive!](https://adventofcode.com/2021/day/2) {#day2}

### Depth sum

Read in the input as a two-column data frame using `read.table()`.
I gave mine nice column names, `cmd` and `value`, but this isn't essential.

Then take advantage of the fact that `TRUE == 1` and `FALSE == 0` to make a mathematical `ifelse`-type statement for the horizontal and vertical movements.
In my R package, this is implemented as a function called [`dive()`](https://github.com/Selbosh/adventofcode2021/blob/main/R/day02.R#L70-L76):

```r
x <- (cmd == 'forward') * value
y <- ((cmd == 'down') - (cmd == 'up')) * value
sum(x) * sum(y)
```

### Cumulative depth sum

Part two is much like part one, but now `y` represents (change in) aim, and (change in) depth is derived from that.
Don't forget the function `cumsum()`, which can save you writing a loop!
Here is the body of my function [`dive2()`](https://github.com/Selbosh/adventofcode2021/blob/main/R/day02.R#L80-L87):

```r
x <- (cmd == 'forward') * value
y <- ((cmd == 'down') - (cmd == 'up')) * value
depth <- cumsum(y) * x
sum(x) * sum(depth)
```

## Day 3 - [Binary Diagnostic](https://adventofcode.com/2021/day/2) {#day3}

### Power consumption

There are a few different ways you could approach part one, but my approach was first to read in the data as a data frame of binary integers using the function `read.fwf()`.
Then, find the most common value in each column using the base function `colMeans()` and rounding the result.

According to the instructions, in the event of a tie you should take 1 to be the most common digit.
Although this is familiar to real life---0.5 rounds up to 1---computers [don't work this way](https://en.wikipedia.org/wiki/Rounding#Round_half_to_even): R rounds to even instead (see `?round`).
Because zero is even, that means `round(0.5)` yields 0.
To get around this, add 1 before rounding, then subtract it again.

My function [`power_consumption()`](https://github.com/Selbosh/adventofcode2021/blob/main/R/day03.R#L78-L81), which once again takes advantage of `TRUE` being equivalent to 1 and `FALSE` to 0:

```r
common <- round(colMeans(x) + 1) - 1
binary_to_int(common) * binary_to_int(!common)
```

To convert a vector of binary digits to decimal, I use the following [utility function](https://github.com/Selbosh/adventofcode2021/blob/main/R/day03.R#L104-L106):

```r
binary_to_int <- function(x) {
  sum(x * 2 ^ rev(seq_along(x) - 1))
}
```

However, if using a string representation then there's a handy function in base R called `strtoi()` that you could also use for this ([thanks to Riinu Pius for that tip](https://twitter.com/_Riinu_/status/1466681283887648769)).

### Life support

Part two finds the common digits in a successively decreasing set of binary numbers.
A loop is appropriate here, since we can halt once there is only one number left.
As this loop will only run (at most) 12 times in total, it shouldn't be too slow in R.

Function [`life_support()`](https://github.com/Selbosh/adventofcode2021/blob/main/R/day03.R#L85-L98):

```r
life_support <- function(x) {
  oxygen <- co2 <- x
  for (j in 1:ncol(x)) {
    if (nrow(oxygen) > 1) {
      common <- most_common(oxygen)
      oxygen <- oxygen[oxygen[, j] == common[j], ]
    }
    if (nrow(co2) > 1) {
      common <- most_common(co2)
      co2 <- co2[co2[, j] != common[j], ]
    }
  }
  binary_to_int(oxygen) * binary_to_int(co2)
}
```

There might be cleverer ways of doing this.

## Day 4 - [Giant Squid](https://adventofcode.com/2021/day/4) {#day4}

### Bingo

This is one of those problems where half the battle is working out which data structure to use.
I wrote a function [`read_draws()`](https://github.com/Selbosh/adventofcode2021/blob/main/R/day04.R#L80-L84) that reads in the first line of the file to get the drawn numbers, then separately reads in the remainder of the file to get the bingo cards stacked as a data frame.
Later we take advantage of the fact that the bingo cards are square to split the data frame into a list of matrices.

```r
read_draws <- function(file) {
  draws <- scan(file, sep = ',', nlines = 1, quiet = TRUE)
  cards <- read.table(file, skip = 1)
  list(draws = draws, cards = cards)
}
```

As numbers are called out, I replace them in the dataset with `NA`s.
Then the helper [`score_card()`](https://github.com/Selbosh/adventofcode2021/blob/main/R/day04.R#L86-L91) counts the number of `NA`s in each row and column.
If there are not enough, we return zero, else we calculate the score.

```r
score_card <- function(mat, draw) {
  marked <- is.na(mat)
  if (all(c(rowMeans(marked), colMeans(marked)) != 1))
    return(0)
  sum(mat, na.rm = TRUE) * draw
}
```

Then we put it all together, looping through the draws, replacing numbers with `NA` and halting as soon as someone wins.
Function [`play_bingo()`](https://github.com/Selbosh/adventofcode2021/blob/main/R/day04.R#L98-L111) is defined as follows, using just base R commands:

```r
play_bingo <- function(draws, cards) {
  size <- ncol(cards)
  ncards <- nrow(cards) / size
  ids <- rep(1:ncards, each = size)

  for (d in draws) {
    cards[cards == d] <- NA
    score <- sapply(split(cards, ids), score_card, draw = d)
    if (any(score > 0))
      return(score[score > 0])
  }
}
```

### Last caller

Part two is very similar, but we throw away each winning bingo card as we go to avoid redundant computation, eventually returning the score when there is only one left.
Here is function [`play_bingo2()`](https://github.com/Selbosh/adventofcode2021/blob/main/R/day04.R#L115-L131), which uses the same two utility functions:

```r
play_bingo2 <- function(draws, cards) {
  size <- ncol(cards)

  for (d in draws) {
    ncards <- nrow(cards) / size
    ids <- rep(1:ncards, each = size)
    cards[cards == d] <- NA
    score <- sapply(split(cards, ids), score_card, draw = d)
    if (any(score > 0)) {
      if (ncards == 1)
        return(score[score > 0])
      cards <- cards[ids %in% which(score == 0), ]
    }
  }
}
```

Further optimizations are possible.
For example: as written, we calculate every intermediate winner's score, but we only really need to do it for the first (part 1) and last (part 2) winners.

Also, we could draw more than one number at a time, as we know that nobody's going to win until at least the fifth draw (for 5&times;5 cards) and from there, increment according to the minimum number of unmarked numbers on any row or column.

I didn't bother implementing either of these, as it already runs quickly enough.

## Day 5 - [Hydrothermal Venture](https://adventofcode.com/2021/day/5) {#day5}

For a while I tried to think about clever mathematical ways to solve the system of inequalities, but this gets complicated when working on a grid, and where some segments are collinear.
In the end it worked out quicker to what seems like a 'brute force' approach:
generate all the points on the line segments and then simply count how many times they appear.

This is a problem that really lends itself to use of **tidyr** functions like [`separate()`](https://tidyr.tidyverse.org/reference/separate.html) and [`unnest()`](https://tidyr.tidyverse.org/reference/nest.html), so naturally I made life harder for myself by doing it in base R, instead.

First, read in the coordinates as a data frame with four columns, `x1`, `y1`, `x2` and `y2`.
The _nice_ way to do this is with `tidyr::separate()` but `strsplit()` works just fine too.
Here is my parsing function, [`read_segments()`](https://github.com/Selbosh/adventofcode2021/blob/main/R/day05.R#L77-L82):

```r
read_segments <- function(x) {
  lines <- do.call(rbind, strsplit(readLines(x), '( -> |,)'))
  storage.mode(lines) <- 'numeric'
  colnames(lines) <- c('x1', 'y1', 'x2', 'y2')
  as.data.frame(lines)
}
```

This is one of the few puzzles where the solution to part two is essentially contained in part one.
Depending on how you implement your home-rolled `unnest`-like function, it could just be a case of filtering out the diagonal lines in part one.
I make liberal use of `mapply` for looping over two vectors at once.

In the penultimate line, we take advantage of vector broadcasting, which handles all the horizontal and vertical lines where you have multiple coordinates on one axis paired with a single coordinate on the other.
For the diagonal lines, there is a 1:1 relationship so the coordinates just bind together in pairs.
Finally, we work out how to count the rows, without using `dplyr::count()`.
If you convert to a data frame, then `table()` does this for you.

```r
count_intersections <- function(lines, part2 = FALSE) {
  if (!part2)
    lines <- subset(lines, x1 == x2 | y1 == y2)
  x <- mapply(seq, lines$x1, lines$x2)
  y <- mapply(seq, lines$y1, lines$y2)
  xy <- do.call(rbind, mapply(cbind, x, y))
  sum(table(as.data.frame(xy)) > 1)
}
```

I'm fairly pleased to get the main solution down to [essentially four lines of code](https://github.com/Selbosh/adventofcode2021/blob/main/R/day05.R#L89-L96), though I'm certain that there are more computationally efficient ways of tackling this problem---if you value computer time more than your own time.

For the tidyverse approach, see [David Robinson's solution](https://twitter.com/drob/status/1467361848525787138).

## Day 6 - [Lanternfish](https://adventofcode.com/2021/day/6) {#day6}

In this problem, we have many fish with internal timers.
As the instructions suggest, we will have exponential growth, so it's not a good idea to keep track of each individual fish as you'll soon run out of memory.
On the other hand, there are only nine possible states for any given fish to be in: the number of days until they next reproduce.
So we can store a vector that simply tallies the number of fish in each state.

On each day, we can shuffle the fish along the vector, decreasing the number of days for each group of fish by 1, and adding new cohorts of fish at day 6, to represent parent fish resetting their timers, and at day 8 to represent the newly hatched lanternfish.
My short function [`lanternfish()`](https://github.com/Selbosh/adventofcode2021/blob/main/R/day06.R#L72-L77):

```r
lanternfish <- function(x, days = 80) {
  fish <- as.double(table(factor(x, levels = 0:8)))
  for (i in 1:days)
    fish <- c(fish[2:7], fish[8] + fish[1], fish[9], fish[1])
  sum(fish)
}
```

Because R indexes from 1 rather than 0, the element `fish[1]` represents the number of fish with 0 days left, `fish[2]` represents the number with 1 day left, and so on.
If you find this confusing, you can index from zero instead, thanks to the new [**index0** package](https://github.com/Selbosh/index0):

```r
lanternfish0 <- function(x, days = 80) {
  fish <- as.double(table(factor(x, levels = 0:8)))
  for (i in 1:days) {
    fish <- index0::index_from_0(fish)
    fish <- c(fish[1:6], fish[7] + fish[0], fish[8], fish[0])
   }
  sum(fish)
}
```

There is a slightly different way to perform the updates.
[David Robinson suggested](https://twitter.com/drob/status/1467727330663534594) an approach based on linear algebra.
Here we apply the same procedure as above, but via matrix multiplication.
It takes about the same time to run.

```r
lanternfish <- function(x, days = 80) {
  fish <- table(factor(x, levels = 0:8))
  mat <- matrix(0, 9, 9)
  mat[cbind(2:9, 1:8)] <- 1 # decrease timer for fish w/ 1-8 days left
  mat[1, c(7, 9)] <- 1      # add 'new' fish with 6 & 8 days left
  for (i in 1:days)
    fish <- fish %*% mat
  sum(fish)
}
```

Day 6 is another puzzle where the solutions for parts one and two are essentially the same.
The only thing to be careful of on part two is that you don't run into integer overflow.
If you do, make sure the numbers you're adding together are of type `double`.

## Day 7 - [The Treachery of Whales](https://adventofcode.com/2021/day/7) {#day7}

### Median

While it's possible to brute-force this puzzle by simply calculating the fuel requirement at every single point (within the range of the inputs), you can do it about 200&times; faster by treating it as an optimization problem.

The total fuel required for any potential position is
```r
x <- scan('input.txt', sep = ',')
f <- function(pos) sum(abs(x - pos))
```
where `x` are the initial locations of the crabs.
Then run it through `optimize()`, and round to the nearest integer position:

```r
sol <- optimize(f, range(x))$minimum
f(round(sol))
```

However, there is an even faster analytical solution!

```r
sol <- median(x)
```

Thanks to [Claire Little](https://twitter.com/claire_little1) for pointing this out.

### Mean

Part two just has a slightly different function to optimize.
Using the formula for the sum of an [arithmetic progression](https://en.wikipedia.org/wiki/Arithmetic_progression):
```r
f2 <- function(pos) {
  n <- abs(x - pos)
  sum(n / 2 * (1 + n))
}
```

Then we can simply minimize this function as before.

```r
sol <- optimize(f2, range(x))$minimum
f2(round(sol))
```

However, there's a shortcut for this part as well!
Calculate the mean of the initial positions, and work out which of the two nearest integers gives the minimum result:

```r
min(
  f2(floor(mean(x))),
  f2(ceiling(mean(x)))
)
```

Thanks to [Jonatan Pallesen](https://twitter.com/jonatanpallesen/status/1468165025575624704).
This is about 5 times faster than my optimizer.

And here is what the functions look like for my input dataset:

```{r day7, echo = FALSE, fig.width = 6, fig.height = 3, dev = 'png', dev.args = list(png = list(type = 'cairo'))}
x <- scan(adventofcode2021::input_file(7), sep = ',', quiet = TRUE)
f1 <- function(pos) sum(abs(x - pos))
f2 <- function(pos) {
  n <- abs(x - pos)
  sum(n / 2 * (1 + n))
} 
day7 <- data.frame(position = min(x):max(x),
                   fuel1 = sapply(min(x):max(x), f1),
                   fuel2 = sapply(min(x):max(x), f2))
library(ggplot2)
theme_set(theme_classic())
day7part1gg <- ggplot(day7) +
  aes(position, fuel1) +
  geom_line() +
  geom_vline(xintercept = median(x), linetype = 'dashed') +
  labs(x = 'x', y = 'fuel', title = 'Part 1') +
  geom_rug(data = data.frame(position = x, fuel1 = mean(day7$fuel1)),
           sides = 'b', outside = TRUE,
           alpha = .1, colour = 'tomato2') +
  scale_y_continuous(breaks = NULL) +
  scale_x_continuous(breaks = range(x)) +
  coord_cartesian(clip = "off")
day7part2gg <- ggplot(day7) +
  aes(position, fuel2) +
  geom_line() +
  geom_vline(xintercept = mean(x), linetype = 'dashed') +
  labs(x = 'x', y = NULL, title = 'Part 2') +
  geom_rug(data = data.frame(position = x, fuel2 = mean(day7$fuel2)),
           sides = 'b', outside = TRUE,
           alpha = .1, colour = 'tomato2') +
  scale_y_continuous(breaks = NULL) +
  scale_x_continuous(breaks = range(x)) +
  coord_cartesian(clip = "off")
gridExtra::grid.arrange(day7part1gg, day7part2gg, ncol = 2)
```

## Day 8 - [Seven Segment Search](https://adventofcode.com/2021/day/8) {#day8}

### Unique digits

Read in the data and then the first part is just a one-liner:

```r
input <- do.call(rbind, strsplit(readLines(input_file(8)), '[^a-z]+'))

count_unique <- function(x) {
  sum(nchar(x[, -(1:10)]) %in% c(2, 3, 4, 7))
}
```

### Segment matching

I _really_ wanted to solve part two using graph theory, by representing the puzzle as a maximum bipartite matching problem.
However, I couldn't quite get this to work.
My final solution is instead just a lot of leg work.

Essentially you solve the problem by hand and then encode the process programmatically.
Recognize that some digits have segments in common, or not in common, and use this to eliminate the possibilities.
I stored the solutions in a named vector, which I was able to use to look up the digits found so far.

The function `setdiff()` comes in useful.

```r
contains <- function(strings, letters) {
  vapply(strsplit(strings, ''),
         function(s) all(strsplit(letters, '')[[1]] %in% s),
         logical(1))
}

output_value <- function(vec) {
  segments <- c('abcefg', 'cf', 'acdeg', 'acdfg', 'bcdf',
                'abdfg', 'abdefg', 'acf', 'abcdefg', 'abcdfg')
  nchars <- setNames(nchar(segments), 0:9)

  # Sort the strings
  vec <- sapply(strsplit(vec, ''), function(d) paste(sort(d), collapse = ''))
  sgn <- head(vec, 10)
  out <- tail(vec, 4)

  # Store the known values
  digits <- setNames(character(10), 0:9)
  unique <- c('1', '4', '7', '8')
  digits[unique] <- sgn[match(nchars[unique], nchar(sgn))]

  # Remaining digits have 5 or 6 segments:
  sgn <- setdiff(sgn, digits)
  digits['3'] <- sgn[nchar(sgn) == 5 & contains(sgn, digits['1'])]
  digits['6'] <- sgn[nchar(sgn) == 6 & !contains(sgn, digits['1'])]
  sgn <- setdiff(sgn, digits)
  digits['0'] <- sgn[nchar(sgn) == 6 & !contains(sgn, digits['4'])]
  sgn <- setdiff(sgn, digits)
  digits['9'] <- sgn[nchar(sgn) == 6]
  sgn <- setdiff(sgn, digits)
  digits['2'] <- sgn[
    contains(sgn, do.call(setdiff,
                          unname(strsplit(digits[c('8', '6')], ''))))
  ]
  digits['5'] <- setdiff(sgn, digits)

  # Combine four output digits:
  as.numeric(paste(match(out, digits) - 1, collapse = ''))
}
```

## Day 9 - [Smoke Basin](https://adventofcode.com/2021/day/9) {#day9}

### Lowest points

You can find all the lowest points with a one-liner:

```r
lowest <- function(h) {
  h < cbind(h, Inf)[, -1] &          # right
    h < rbind(h, Inf)[-1, ] &        # down
    h < cbind(Inf, h[, -ncol(h)]) &  # left
    h < rbind(Inf, h[-nrow(h), ])    # up
}
```

Then do `sum(h[lowest(h)])` to get the result, where `h` is a numeric matrix of the input data.

### Basins

The second part is harder and doesn't immediately lead from the first.
Initially I thought of replacing each lowest point with `Inf`, then finding the new lowest points and repeating the process until all the basins are found.
However, the basins are simply all points where the height is `< 9`, so you can find the basins in a single step.

The tricky part is labelling them separately, so you can count up their respective sizes.

The boring way of doing this is just to loop over the indices and label the points that neighbour already-labelled ones (starting with the lowest points as the initial labels), doing several passes until everything (except the 9s) is labelled.

```r
basins <- function(h) {
  l <- lowest(h)
  h[] <- ifelse(h < 9, NA, Inf)
  h[l] <- 1:sum(l)
  while (anyNA(h)) {
    for (i in 1:nrow(h)) for (j in 1:ncol(h)) {
      if (is.na(h[i, j])) {
        nbrs <- h[cbind(c(max(i - 1, 1), min(i + 1, nrow(h)), i, i),
                        c(j, j, max(j - 1, 1), min(j + 1, ncol(h))))]
        if (any(is.finite(nbrs)))
          h[i, j] <- nbrs[is.finite(nbrs)][1]
      }
    }
  }
  sizes <- table(h[is.finite(h)])
  head(sort(sizes, decreasing = TRUE), 3)
}
```

To vectorize this in the same way as part one, we define a new binary (infix) operator `%c%`, analogous to `dplyr::coalesce()`.
What this does is replace an `NA` value (a basin not yet assigned a label) with its finite neighbour, while leaving `Inf`s (marking basin edges) alone.

```r
"%c%" <- function(x, y) {
  ifelse(is.infinite(x), x,
         ifelse(!is.na(x), x,
                ifelse(!is.infinite(y), y, x)))
}
```

Then the new function for part two is as follows.
It is five times faster to run than the nested loop above.

```r
basins2 <- function(h) {
  l <- lowest(h)
  h[] <- ifelse(h < 9, NA, Inf)
  h[l] <- 1:sum(l)
  while(anyNA(h)) {
    h <- h %c%
      cbind(h, NA)[, -1] %c%        # right
      rbind(h, NA)[-1, ] %c%        # down
      cbind(NA, h[, -ncol(h)]) %c%  # left
      rbind(NA, h[-nrow(h), ])      # up
  }
  sizes <- table(h[is.finite(h)])
  head(sort(sizes, decreasing = TRUE), 3)
}
```

You can also [formulate this as an image analysis problem](https://twitter.com/rappa753/status/1468876602016735233), effectively treating each basin as an area of similar colour to select, or you can [treat it as a network theory problem and apply the **igraph** package](https://twitter.com/babeheim/status/1468898580408811525) to find graph components.

## Day 10 - [Syntax Scoring](https://adventofcode.com/2021/day/10) {#day10}

### Corrupt characters

Whilst it's probably possible to do the first part with some very fancy [recursive regular expressions](https://www.php.net/manual/en/regexp.reference.recursive.php), I don't know how to use them.

Instead, my method of finding unmatched brackets is simply to search for empty pairs of brackets and successively strip them from the string.
Keep doing this until the strings stop changing.
Then, get the first closing bracket (if any), using `regmatches()`.
These are the illegal characters.

My function [`syntax_score()`](https://github.com/Selbosh/adventofcode2021/blob/main/R/day10.R#L122-L128) is implemented as follows:

```r
lines <- readLines('input.txt')
old <- ''
while (!identical(old, lines -> old))
  lines <- gsub(r'(\(\)|<>|\{\}|\[\])', '', lines)
illegals <- regmatches(lines, regexpr(r'(\)|>|\}|\])', lines))
```
The syntax score is calculated using a named vector as a lookup table.
```r
illegal_score <- c(')' = 3, ']' = 57, '}' = 1197, '>' = 25137)
sum(illegal_score[illegals])
```

### Autocomplete

Part two starts the same, but instead of extracting the illegal characters we just throw away those lines that contain them.

```r
illegals <- grep(r'(\)|>|\}|\])', lines)
chars <- strsplit(lines[-illegals], '')
```
From here, we can calculate the scores using a `Reduce` operation (from right to left) with another lookup table.
The final answer is the median score.
```r
complete_score <- c('(' = 1, '[' = 2, '{' = 3, '<' = 4)
scores <- sapply(chars, Reduce, init = 0, right = TRUE,
                 f = \(c, s) 5 * s + complete_score[c])
median(scores)
```
The function [`autocomplete()`](https://github.com/Selbosh/adventofcode2021/blob/main/R/day10.R#L132-L141) wraps it all together.

## Day 11 - [Dumbo Octopus](https://adventofcode.com/2021/day/11) {#day11}

### Convoluted octopuses

The process of updating the energy levels can be described using a [_convolution matrix_](https://en.wikipedia.org/wiki/Kernel_(image_processing)).
It's easy---[like on Day 11 last year](https://selbydavid.com/2020/12/06/advent-2020/#day11)---to use a ready-made solution from an image analysis package for this, namely `OpenImageR::convolution()`.

The convolution matrix, or _kernel_ is
\[\begin{bmatrix}1  &  1  &  1 \\1  &   0  &  1 \\1  &  1  &  1\end{bmatrix},\]
to be applied on an indicator matrix of 'flashing' octopuses, and then added to the result.
In R,
```r
kernel <- matrix(1:9 != 5, 3, 3)
```

So we define a function, `step1()`, that applies a single step of the energy level updating process.
Since each octopus can only flash once in a given step, we keep track of those that have already flashed, as well as those currently flashing.
A short `while()` loop repeats until no more octopuses flash.

```r
step1 <- function(x) {
  x <- x + 1
  flashing <- flashed <- x == 10
  while (any(flashing)) {
    x <- x + OpenImageR::convolution(flashing, kernel)
    flashing <- x > 9 & !flashed
    flashed <- flashing | flashed
  }
  x[x > 9] <- 0
  x
}
```

However, there is a base R alternative to `OpenImageR::convolution()` that you can substitute in, for negligible speed penalty (despite that package being written in C).

```r
add_neighbours <- function(x) {
  I <- nrow(x)
  J <- ncol(x)
  cbind(x[, -1], 0) +  # Right
  rbind(x[-1, ], 0) +  # Down
  cbind(0, x[, -J]) +  # Left
  rbind(0, x[-I, ]) +  # Up
  rbind(cbind(x[-1, -1], 0), 0) + # SE
  rbind(0, cbind(x[-I, -1], 0)) + # NE
  rbind(cbind(0, x[-1, -J]), 0) + # SW
  rbind(0, cbind(0, x[-I, -J]))   # NW
}
```

### Counting flashes

We then solve both parts one and two with a single function that repeatedly applies the updating step and counts the flashes (number of zeros) each time.

By default, it simply completes `iter` steps and then returns cumulative total number of flashes.

In `part2` mode, the function will terminate early if it encounters a step where all the octopuses are flashing and return the iteration number, otherwise it will generate a warning.

```r
count_flashes <- function(x, iter = 100, part2 = FALSE) {
  count <- 0
  for (i in 1:iter) {
    x <- step1(x)
    nflashes <- sum(x == 0)
    if (part2 & nflashes == prod(dim(x)))
      return(i)
    count <- count + nflashes
  }
  if (part2)
    warning('No synchronization detected in ', iter, ' steps')
  count
}
```

The whole thing runs in about 20 milliseconds on my input dataset, or 60 milliseconds using the base R convolution function.
