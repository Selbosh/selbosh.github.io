<!DOCTYPE html>
<html lang="en-GB">
  <head>
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<meta name="author" content="David Selby">

  


<title>Building a neural network from scratch in R &middot; Tea &amp; Stats</title>
<meta property="og:title" content="Building a neural network from scratch in R" />
<meta property="og:description" content="Neural networks can seem like a bit of a black box.But in some ways, a neural network is little more than several logistic regression models chained together.
In this post I will show you how to derive a neural network from scratch with just a few lines in R.If you don’t like mathematics, feel free to skip to the code chunks towards the end.
This blog post is partly inspired by Denny Britz’s article, Implementing a Neural Network from Scratch in Python, as well as this article by Sunil Ray." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://selbydavid.com/2018/01/09/neural-network/" />

  <meta property="og:image" content="https://selbydavid.com/img/2018/hotdog.jpg" />



<meta property="article:published_time" content="2018-01-09T10:00:00&#43;00:00"/>

<meta property="article:modified_time" content="2018-01-09T10:00:00&#43;00:00"/>













<meta itemprop="name" content="Building a neural network from scratch in R">
<meta itemprop="description" content="Neural networks can seem like a bit of a black box.But in some ways, a neural network is little more than several logistic regression models chained together.
In this post I will show you how to derive a neural network from scratch with just a few lines in R.If you don’t like mathematics, feel free to skip to the code chunks towards the end.
This blog post is partly inspired by Denny Britz’s article, Implementing a Neural Network from Scratch in Python, as well as this article by Sunil Ray.">


<meta itemprop="datePublished" content="2018-01-09T10:00:00&#43;00:00" />
<meta itemprop="dateModified" content="2018-01-09T10:00:00&#43;00:00" />
<meta itemprop="wordCount" content="3484">

  <meta itemprop="image" content="https://selbydavid.com/img/2018/hotdog.jpg">



<meta itemprop="keywords" content="" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image:src" content="https://selbydavid.com/img/2018/hotdog.jpg"/>
<meta name="twitter:title" content="Building a neural network from scratch in R"/>
<meta name="twitter:description" content="Neural networks can seem like a bit of a black box.But in some ways, a neural network is little more than several logistic regression models chained together.
In this post I will show you how to derive a neural network from scratch with just a few lines in R.If you don’t like mathematics, feel free to skip to the code chunks towards the end.
This blog post is partly inspired by Denny Britz’s article, Implementing a Neural Network from Scratch in Python, as well as this article by Sunil Ray."/>
<meta name="twitter:site" content="@TeaStats"/>



  
    <meta property="article:author" content="https://www.facebook.com/Selbosh" />
    <meta property="article:publisher" content="https://www.facebook.com/Selbosh" />
  
  
    <meta name="twitter:creator" content="@TeaStats"/>
  


<meta name="theme" content="hugo-tea">
<meta name="generator" content="Hugo 0.32.2" />





<link rel="shortcut icon" href="https://selbydavid.com/favicon.ico">




<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="https://selbydavid.com/css/main.css" media="all">

<script src="https://use.fontawesome.com/269c7751ba.js"></script>

  </head>
  <body>
      
        <header id="masthead">
          <img src="https://selbydavid.com/img/logo.svg"
               width="120"
               height=""
               alt="Tea and Stats">
          <h1><a href="https://selbydavid.com/">Tea &amp; Stats</a></h1>
          <aside>Data science with David Selby</aside>
          <nav>
  <ul>
    
      <li><a href="https://selbydavid.com/about/">About</a></li>
    
    
    <li>
      <a href="https://facebook.com/Selbosh">
        <i class="fa fa-facebook"></i>
        
      </a>
    </li>
    <li>
      <a href="https://github.com/Selbosh">
        <i class="fa fa-github"></i>
        
      </a>
    </li>
    <li>
      <a href="https://linkedin.com/in/daselby">
        <i class="fa fa-linkedin"></i>
        
      </a>
    </li>
    <li>
      <a href="https://twitter.com/TeaStats">
        <i class="fa fa-twitter"></i>
        
      </a>
    </li>
  </ul>
</nav>

        </header>


<main role="main">
  <article class="single">
    <header><h2>Building a neural network from scratch in R</h2></header>
    
    <footer>
      <time datetime="2018-01-09 10:00:00 &#43;0000">
         9 January 2018
      </time>
    </footer>
    
    <section>
    <script src="https://selbydavid.com/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<link href="https://selbydavid.com/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="https://selbydavid.com/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>


<p>Neural networks can seem like a bit of a black box.
But in some ways, a neural network is little more than several <a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression</a> models chained together.</p>
<p>In this post I will show you how to derive a neural network from scratch with just a few lines in R.
If you don’t like mathematics, feel free to skip to the code chunks towards the end.</p>
<p>This blog post is partly inspired by Denny Britz’s article, <a href="http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/">Implementing a Neural Network from Scratch in Python</a>, as well as <a href="https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/">this article by Sunil Ray</a>.</p>
<div id="logistic-regression" class="section level2">
<h2>Logistic regression</h2>
<p>What’s a logistic regression model?
Suppose we want to build a machine that classifies objects in two groups, for example <a href="https://www.youtube.com/watch?v=ACmydtFDTGs">‘hot dog’ and ‘not hot dog’</a>.
We will say <span class="math inline">\(Y_i = 1\)</span> if object <em>i</em> is a hot dog, and <span class="math inline">\(Y_i = 0\)</span> otherwise.</p>
<p><a href="https://www.youtube.com/watch?v=ACmydtFDTGs"><img src="https://selbydavid.com/img/2018/hotdog.gif" /></a></p>
<p>A logistic regression model estimates these odds,
<span class="math display">\[
\operatorname{odds}(Y = 1)
= \frac{\operatorname P(Y = 1)} {\operatorname P(Y = 0)}
= \frac{\operatorname P(Y = 1)} {\operatorname 1 - \operatorname P(Y = 1)}
\]</span>
and for mathematical and computational reasons we take the natural logarithm of the same—the log odds.
As a <a href="https://en.wikipedia.org/wiki/Generalized_linear_model">generalised linear model</a>, the response (the log odds) is a linear combination of the parameters and the covariates,
<span class="math display">\[\operatorname{log-odds}(Y = 1) = X \beta,\]</span>
where <span class="math inline">\(X\)</span> is the <a href="https://en.wikipedia.org/wiki/Design_matrix">design matrix</a> and <span class="math inline">\(\beta\)</span> is a vector of parameters to be found.</p>
<p>Classical statistical inference involves looking for a parameter estimate, <span class="math inline">\(\hat\beta\)</span>, that <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">maximises the likelihood</a> of the observations given the parameters.
For our hot dog classification problem, the likelihood function is
<span class="math display">\[\mathcal{L} = \prod_i \operatorname P(Y_i=1)^{y_i} \operatorname P(Y_i = 0)^{1 - y_i}\]</span>
or, taking logs,
<span class="math display">\[\log \mathcal{L} = \sum_i \Bigl[ y_i \log \operatorname P(Y_i = 1) + (1-y_i) \log \operatorname P(Y_i = 0) \Bigr]. \]</span></p>
<p>Let’s imagine we have been given some two-dimensional data about a sample of objects, along with their labels.
Our sample data set includes 200 observations.
Here is a random sample of five:</p>
<table>
<thead>
<tr class="header">
<th align="right">x1</th>
<th align="right">x2</th>
<th align="left">class</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">8.81</td>
<td align="right">-6.19</td>
<td align="left">not hot dog</td>
</tr>
<tr class="even">
<td align="right">10.00</td>
<td align="right">0.84</td>
<td align="left">not hot dog</td>
</tr>
<tr class="odd">
<td align="right">-1.78</td>
<td align="right">4.48</td>
<td align="left">hot dog</td>
</tr>
<tr class="even">
<td align="right">6.83</td>
<td align="right">-0.02</td>
<td align="left">hot dog</td>
</tr>
<tr class="odd">
<td align="right">1.71</td>
<td align="right">-10.29</td>
<td align="left">not hot dog</td>
</tr>
</tbody>
</table>
<p>And a scatter plot of all of them:</p>
<p><img src="https://selbydavid.com/post/2018-01-09-neural-network_files/figure-html/plot-spirals-1.png" width="672" /></p>
<p>Fitting a logistic regression model in R is easy:</p>
<pre class="r"><code>logreg &lt;- glm(class ~ x1 + x2, family = binomial, data = hotdogs)</code></pre>
<p>But as the decision boundary can only be linear, it doesn’t work too well at distinguishing our two classes.
Logistic regression classifies 134 (67%) of our objects correctly.</p>
<p><img src="https://selbydavid.com/post/2018-01-09-neural-network_files/figure-html/logistic-boundary-1.png" width="672" /></p>
<p>Nonlinearity is where neural networks are useful.</p>
</div>
<div id="artificial-neural-networks" class="section level2">
<h2>Artificial neural networks</h2>
<p>An <a href="https://en.wikipedia.org/wiki/Artificial_neural_network">artificial neural network</a> is so called because once upon a time it was thought to be a good model for how neurons in the brain work.
Apparently it isn’t, but the name has stuck.</p>
<p>Suppose we have some inputs, <span class="math inline">\(\mathbf x\)</span>, and known outputs <span class="math inline">\(\mathbf y\)</span>.
Then the aim of the game is to find a way of estimating <span class="math inline">\(\mathbf y\)</span> based on <span class="math inline">\(\mathbf x\)</span>.
In a way, then, a neural network is like any other regression or classification model.</p>
<p>Neural networks (for classification, at least) are also known as multi-layer <a href="https://en.wikipedia.org/wiki/Perceptron">perceptrons</a>.
Like ogres and onions, they have layers—three to be exact.</p>
<p>The output layer is our estimate of the probability that objects belong to each class.
The input layer comprises the covariates and an intercept, as before.
In the middle, there is a <em>hidden layer</em>, which is a transformation of the input space into <span class="math inline">\(h\)</span> dimensions, where <span class="math inline">\(h\)</span> is a number chosen by us.
We then perform a logistic regression on this transformed space to estimate the classes.</p>
<p><img src="https://selbydavid.com/post/2018-01-09-neural-network_files/figure-html/neural-network-1.svg" width="672" /></p>
<p>It works like this.</p>
<ol style="list-style-type: decimal">
<li>Generate <span class="math inline">\(h\)</span> different linear combinations of the input variables.</li>
<li>Apply an ‘activation’ function, that for each observation, turns each hidden node ‘on’ or ‘off’.</li>
<li>Fit a logistic regression model to these <span class="math inline">\(h\)</span> transformed predictors, plus an intercept.</li>
<li>Adjust the parameters of both the input and the output to maximise likelihood.</li>
<li>Repeat, <em>ad nauseum</em>.</li>
</ol>
<p>If <span class="math inline">\(h = 1\)</span> then there is only one linear combination of the predictors, which is effectively the same thing as having no hidden layer at all, i.e. ordinary logistic regression.</p>
<p>So we run a kind of logistic regression model on the inputs to generate a transformation of the feature space, then once more to classify our actual objects.
Each iteration of the fitting or ‘training’ process adjusts both the transformation and the regression parameters.</p>
</div>
<div id="hidden-layers" class="section level2">
<h2>Hidden layers</h2>
<p>The input layer is a design matrix, <span class="math inline">\(\mathbf X = \begin{bmatrix}\mathbf 1 &amp; \mathbf x\end{bmatrix}\)</span>.
The output layer is a vector of estimated probabilities, <span class="math inline">\(\hat {\mathbf y}\)</span>.
So far, this is exactly like our logistic regression model above.</p>
<p>A neural network adds a hidden layer, which you might think of as an intermediate design matrix between the inputs and the outputs.
<a href="https://en.wikipedia.org/wiki/Deep_learning">Deep learning</a> is just a neural network with multiple hidden layers.</p>
<div class="figure"><span id="fig:node-diagram"></span>
<img src="https://selbydavid.com/post/2018-01-09-neural-network_files/figure-html/node-diagram-1.svg" alt="Multi-layer perceptron with five hidden nodes" width="672" />
<p class="caption">
Figure 1: Multi-layer perceptron with five hidden nodes
</p>
</div>
<p>If there are <span class="math inline">\(d\)</span> input variables then there are <span class="math inline">\(d+1\)</span> input nodes—one for each covariate, plus an intercept, or ‘bias’ (because computer scientists like having separate names for everything).</p>
<p>In general there are <span class="math inline">\(k-1\)</span> output nodes, where <span class="math inline">\(k\)</span> is the number of possible classes.
The <span class="math inline">\(k^\text{th}\)</span> node would be the same as ‘none of the above’, so it is redundant.
In a binary classification problem like ours, there is just one output node, because <span class="math inline">\(\operatorname P(Y=0) = 1 - \operatorname P(Y=1)\)</span>.</p>
<p>There are <span class="math inline">\(h\)</span> nodes in the hidden layer, plus an intercept.
Each of these is a linear combination of the inputs, passed through an <a href="https://en.wikipedia.org/wiki/Activation_function"><em>activation function</em></a>.
The intercept does not depend on the nodes in the previous layer<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>.</p>
<p>The activation function is often (not always) chosen to be the <em>sigmoid function</em>, another name for the <em>logistic function</em>,
<span class="math display">\[\sigma(x) = \frac 1 {1 + e^{-x}},\]</span>
the inverse of the logit
<span class="math display">\[\operatorname{logit}(x) = \log \left( \frac{x}{1-x} \right).\]</span>
If <span class="math inline">\(x\)</span> is a probability of an event, then <span class="math inline">\(\operatorname{logit}(x)\)</span> is its log odds.</p>
</div>
<div id="forward-propagation" class="section level2">
<h2>Forward propagation</h2>
<p>Starting with the inputs, we <a href="https://en.wikipedia.org/wiki/Feedforward_neural_network">feed forward</a> through the network as follows.</p>
<p>Firstly, compute a linear combination of the covariates, using some weight matrix <span class="math inline">\(\mathbf W_\text{in} \in \mathbb R^{(d+1) \times h}\)</span>.
<span class="math display">\[
\mathbf z_1
= \mathbf{XW}_\text{in}
= \begin{bmatrix}\mathbf 1 &amp; \mathbf x\end{bmatrix} \mathbf W_\text{in}
\]</span>
Next, apply an activation function to obtain the nodes in the hidden layer.
The hidden layer <span class="math inline">\(\mathbf H\)</span> might be thought of as a design matrix containing the output of a logistic regression classifying whether each node is ‘activated’ or not.
<span class="math display">\[\mathbf h = \sigma(\mathbf z_1)\]</span>
The intercept/bias is always activated, so it is fixed to be a vector of ones.
<span class="math display">\[\mathbf H = \begin{bmatrix} \mathbf 1 &amp; \mathbf h \end{bmatrix}
            = \begin{bmatrix} \mathbf 1 &amp; \sigma(\mathbf z_1) \end{bmatrix}
            = \begin{bmatrix} \mathbf 1 &amp; \sigma(\mathbf {XW}_\text{in}) \end{bmatrix}\]</span>
For the output layer, compute a linear combination of the hidden variables, this time using another weight matrix, <span class="math inline">\(\mathbf{W}_\text{out} \in \mathbb R^{(h+1) \times (k-1)}\)</span>.
<span class="math display">\[\mathbf z_2
= \mathbf {HW}_\text{out}
= \begin{bmatrix} \mathbf 1 &amp; \mathbf h\end{bmatrix} \mathbf W_\text{out}
\]</span>
Apply one more function to get the output
<span class="math display">\[\hat {\mathbf y} = \sigma (\mathbf z_2),\]</span>
which is a probability vector, <span class="math inline">\(\hat Y_i = \operatorname P(Y_i = 1)\)</span>.</p>
<p>Putting it all together,
<span class="math display">\[\hat {\mathbf y}
= \sigma \left( \mathbf {H W} _ \text{out} \right)
= \sigma \bigl( \begin{bmatrix} \mathbf 1 &amp; \sigma ( \mathbf {X W} _ \text{in} ) \end{bmatrix} \mathbf W _ \text{out} \bigr).\]</span></p>
<p>It is straightforward to write a function to perform the forward propagation process in R. Just do</p>
<pre class="r"><code>feedforward &lt;- function(x, w1, w2) {
  z1 &lt;- cbind(1, x) %*% w1
  h &lt;- sigmoid(z1)
  z2 &lt;- cbind(1, h) %*% w2
  list(output = sigmoid(z2), h = h)
}</code></pre>
<p>where</p>
<pre class="r"><code>sigmoid &lt;- function(x) 1 / (1 + exp(-x))</code></pre>
<p>Where do we get the weights from?
On the first iteration, they can be random.
Then we have to make them better.</p>
</div>
<div id="back-propagation" class="section level2">
<h2>Back propagation</h2>
<p>So far we have been taking the parameters, or weights, <span class="math inline">\(\mathbf W_\text{in}\)</span> and <span class="math inline">\(\mathbf W_\text{out}\)</span>, for granted.</p>
<p>Like parameters in a linear regression, we need to choose weights that make our model ‘better’ by some criterion.
Neural network enthusiasts will say that we will train our multilayer perceptron by minimising the cross entropy loss.
That’s a fancy way of saying we fit the model using maximum likelihood.</p>
<p>The log-likelihood for a binary classifier is
<span class="math display">\[\ell = \sum_i \Bigl( y_i \log \hat y_i + (1 - y_i) \log (1 - \hat y_i) \Bigr).\]</span>
We can maximise this via <a href="https://en.wikipedia.org/wiki/Gradient_descent">gradient descent</a>, a general-purpose optimisation algorithm.</p>
<p>To minimise <span class="math inline">\(\ell = f(\mathbf W)\)</span> via gradient descent, we iterate using the formula
<span class="math display">\[\mathbf W_{t+1} = \mathbf W_{t} - \gamma \cdot \nabla f(\mathbf W_{t}),\]</span>
where <span class="math inline">\(\mathbf W_t\)</span> is the weight matrix at time <span class="math inline">\(t\)</span>, <span class="math inline">\(\nabla f\)</span> is the gradient of <span class="math inline">\(f\)</span> with respect to <span class="math inline">\(\mathbf W\)</span> and <span class="math inline">\(\gamma\)</span> is the ‘learning rate’.</p>
<p>Choose a learning rate too high and the algorithm will leap around like a dog chasing a squirrel, going straight past the optimum; choose one too low and it will take forever, making mountains out of molehills.</p>
<p>Using the <a href="https://en.wikipedia.org/wiki/Chain_rule">chain rule</a>, the gradient of the log-likehood with respect to the output weights is given by
<span class="math display">\[\frac {\partial\ell} {\partial \mathbf W_\text{out}} =
\frac{\partial \ell}{\partial \hat {\mathbf y}}
\frac{\partial \hat {\mathbf y} }{\partial \mathbf W_\text{out}}\]</span>
where
<span class="math display">\[\begin{aligned}
\frac{\partial\ell}{\partial \hat{\mathbf y}}
  &amp;=  \frac{\mathbf y}{\hat{\mathbf y}} - \frac{1 - \mathbf y}{1 - \hat{\mathbf y}}
  = \frac{\hat{\mathbf y} - \mathbf y}{\hat{\mathbf y}(1 - \hat{\mathbf y})},\\
\frac{\partial\hat{\mathbf y}}{\partial \mathbf W_\text{out}}
  &amp;=  \mathbf{H}^T \sigma(\mathbf {HW}_\text{out})\bigl( 1 - \sigma(\mathbf{HW}_\text{out}) \bigr) \\
  &amp;= \mathbf H^T \hat {\mathbf y} (1 - \hat{\mathbf y}).
\end{aligned}\]</span></p>
<p>And the gradient with respect to the input weights is
<span class="math display">\[
\frac {\partial\ell} {\partial \mathbf W_\text{in}}
= \frac{\partial \ell}{\partial \hat {\mathbf y} }
   \frac{\partial \hat {\mathbf y} }{\partial \mathbf H}
   \frac{\partial \mathbf H}{\partial \mathbf W_\text{in}}
\]</span>
where
<span class="math display">\[
\begin{aligned}
\frac{\partial \hat{\mathbf y}}{\partial \mathbf H}
  &amp;=  \sigma(\mathbf{HW}_\text{out}) \bigl( 1 - \sigma(\mathbf{HW}_\text{out}) \bigr) \mathbf W_\text{out}^T \\
  &amp;= \hat{\mathbf y} (1 - \hat{\mathbf y}) \mathbf W_\text{out}^T, \\
\frac{\partial \mathbf H}{\partial \mathbf W_\text{in}}
  &amp;= \mathbf X^T \begin{bmatrix} \mathbf 0 &amp; \sigma(\mathbf{XW}_\text{in})\bigl( 1 - \sigma(\mathbf{XW}_\text{in}) \bigr) \end{bmatrix}.
\end{aligned}
\]</span></p>
<p>In the last step, we take for granted that the intercept column of <span class="math inline">\(\mathbf H\)</span> doesn’t depend on <span class="math inline">\(\mathbf W_\text{in}\)</span>, but you could parametrise it differently (<a href="#fn1">see footnotes</a>).</p>
<p>A simple R implementation is as follows.</p>
<pre class="r"><code>backpropagate &lt;- function(x, y, y_hat, w1, w2, h, learn_rate) {
  dw2 &lt;- t(cbind(1, h)) %*% (y_hat - y)
  dh  &lt;- (y_hat - y) %*% t(w2[-1, , drop = FALSE])
  dw1 &lt;- t(cbind(1, x)) %*% (h * (1 - h) * dh)
  
  w1 &lt;- w1 - learn_rate * dw1
  w2 &lt;- w2 - learn_rate * dw2
  
  list(w1 = w1, w2 = w2)
}</code></pre>
</div>
<div id="training-the-network" class="section level2">
<h2>Training the network</h2>
<p>Training is just a matter of initialising the weights, propagating forward to get an output estimate, then propagating the error backwards to update the weights towards a better solution.
Then iteratively propagate forwards and backwards until the Earth has been swallowed by the Sun, or some other stopping criterion.</p>
<p>Here is a quick implementation using the functions defined above.</p>
<pre class="r"><code>train &lt;- function(x, y, hidden = 5, learn_rate = 1e-2, iterations = 1e4) {
  d &lt;- ncol(x) + 1
  w1 &lt;- matrix(rnorm(d * hidden), d, hidden)
  w2 &lt;- as.matrix(rnorm(hidden + 1))
  for (i in 1:iterations) {
    ff &lt;- feedforward(x, w1, w2)
    bp &lt;- backpropagate(x, y,
                        y_hat = ff$output,
                        w1, w2,
                        h = ff$h,
                        learn_rate = learn_rate)
    w1 &lt;- bp$w1; w2 &lt;- bp$w2
  }
  list(output = ff$output, w1 = w1, w2 = w2)
}</code></pre>
<p>Let’s <code>train</code> a neural network with five hidden nodes (like in Figure 1) on the hot dogs data.</p>
<pre class="r"><code>x &lt;- data.matrix(hotdogs[, c(&#39;x1&#39;, &#39;x2&#39;)])
y &lt;- hotdogs$class == &#39;hot dog&#39;
nnet5 &lt;- train(x, y, hidden = 5, iterations = 1e5)</code></pre>
<p>On my desktop PC, it takes about 12 seconds for the above code to run the 100,000 iterations.
Not too bad for what sounds like a lot of runs, but what are the results like?</p>
<p>We can calculate how many objects it classified correctly:</p>
<pre class="r"><code>mean((nnet5$output &gt; .5) == y)</code></pre>
<pre><code>## [1] 0.76</code></pre>
<p>That’s 76%, or 152 out of 200 objects in the right class.</p>
<p>Let’s draw a picture to see what the decision boundaries look like.
To do that, we firstly make a grid of points around the input space:</p>
<pre class="r"><code>grid &lt;- expand.grid(x1 = seq(min(hotdogs$x1) - 1,
                             max(hotdogs$x1) + 1,
                             by = .25),
                    x2 = seq(min(hotdogs$x2) - 1,
                             max(hotdogs$x2) + 1,
                             by = .25))</code></pre>
<p>Then feed these points forward through our trained neural network.</p>
<pre class="r"><code>ff_grid &lt;- feedforward(x = data.matrix(grid[, c(&#39;x1&#39;, &#39;x2&#39;)]),
                       w1 = nnet5$w1,
                       w2 = nnet5$w2)
grid$class &lt;- factor((ff_grid$output &gt; .5) * 1,
                     labels = levels(hotdogs$class))</code></pre>
<p>Then, using <code>ggplot2</code>, we plot the predicted classes on a grid behind the observed points.</p>
<pre class="r"><code>ggplot(hotdogs) + aes(x1, x2, colour = class) +
  geom_point(data = grid, size = .5) +
  geom_point() +
  labs(x = expression(x[1]), y = expression(x[2]))</code></pre>
<p><img src="https://selbydavid.com/post/2018-01-09-neural-network_files/figure-html/plot-ad-hoc-1.png" width="672" />
The regions the neural network has classified into ‘hot dog’ and ‘not hot dog’ can no longer be separated by a single straight line.
The more nodes we add to the hidden layer, the more elaborate the decision boundaries can become, improving accuracy at the expense of computation time (as more weights must be calculated) and increased risk of <a href="https://en.wikipedia.org/wiki/Overfitting">over-fitting</a> the data.</p>
<p>How about 30 nodes?</p>
<pre class="r"><code>nnet30 &lt;- train(x, y, hidden = 30, iterations = 1e5)</code></pre>
<p>After 100,000 iterations, accuracy is 100%.
The decision boundary looks much smoother:</p>
<p><img src="https://selbydavid.com/post/2018-01-09-neural-network_files/figure-html/plot-30-nodes-1.png" width="672" /></p>
<p>And for completeness, let’s see what one hidden node (plus an intercept) gives us.</p>
<pre class="r"><code>nnet1 &lt;- train(x, y, hidden = 1, iterations = 1e5)</code></pre>
<p><img src="https://selbydavid.com/post/2018-01-09-neural-network_files/figure-html/plot-1-node-1.png" width="672" /></p>
<p>Much worse accuracy—68%—and the decision boundary looks linear.</p>
</div>
<div id="r6-class-implementation" class="section level2">
<h2>R6 class implementation</h2>
<p>The above code works, mathematically, but isn’t the most elegant solution from a user interface point of view.
It’s a bit ad hoc.
One of the annoying things about writing R functions is that, unless you use the global assignment operator (<code>&lt;&lt;-</code>) then the arguments are <a href="https://en.wikipedia.org/wiki/Immutable_object">immutable</a>.</p>
<p>Also, everything defined within the function scope is discarded.
You can return the objects in a list, but this can be unwieldy and you have to extract each object one by one to pass into the arguments of the next function.
And despite this we still have three functions and various objects cluttering the workspace.</p>
<p>Can we do better?
A more flexible implementation is object orientated, using <a href="https://cran.r-project.org/package=R6"><code>R6</code> classes</a>.</p>
<p>The following class not only supports binary classification but also <a href="https://en.wikipedia.org/wiki/Multinomial_logistic_regression">multinomial logistic regression</a>, providing a high-level formula interface like that used when fitting an <code>lm</code> or <code>glm</code> with the <code>stats</code> package.</p>
<p>The maths for multi-class classification is very similar (partly thanks to the derivative of the <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax function</a>—the multivariate generalisation of sigmoid—cancelling out in the gradient calculation) and most of the modifications are to do with wrangling R factors to and from indicator matrix representation.</p>
<pre class="r"><code>library(R6)
NeuralNetwork &lt;- R6Class(&quot;NeuralNetwork&quot;,
  public = list(
    X = NULL,  Y = NULL,
    W1 = NULL, W2 = NULL,
    output = NULL,
    initialize = function(formula, hidden, data = list()) {
      # Model and training data
      mod &lt;- model.frame(formula, data = data)
      self$X &lt;- model.matrix(attr(mod, &#39;terms&#39;), data = mod)
      self$Y &lt;- model.response(mod)
      
      # Dimensions
      D &lt;- ncol(self$X) # input dimensions (+ bias)
      K &lt;- length(unique(self$Y)) # number of classes
      H &lt;- hidden # number of hidden nodes (- bias)
      
      # Initial weights and bias
      self$W1 &lt;- .01 * matrix(rnorm(D * H), D, H)
      self$W2 &lt;- .01 * matrix(rnorm((H + 1) * K), H + 1, K)
    },
    fit = function(data = self$X) {
      h &lt;- self$sigmoid(data %*% self$W1)
      score &lt;- cbind(1, h) %*% self$W2
      return(self$softmax(score))
    },
    feedforward = function(data = self$X) {
      self$output &lt;- self$fit(data)
      invisible(self)
    },
    backpropagate = function(lr = 1e-2) {
      h &lt;- self$sigmoid(self$X %*% self$W1)
      Yid &lt;- match(self$Y, sort(unique(self$Y)))
      
      haty_y &lt;- self$output - (col(self$output) == Yid) # E[y] - y
      dW2 &lt;- t(cbind(1, h)) %*% haty_y
      
      dh &lt;- haty_y %*% t(self$W2[-1, , drop = FALSE])
      dW1 &lt;- t(self$X) %*% (self$dsigmoid(h) * dh)
      
      self$W1 &lt;- self$W1 - lr * dW1
      self$W2 &lt;- self$W2 - lr * dW2
      
      invisible(self)
    },
    predict = function(data = self$X) {
      probs &lt;- self$fit(data)
      preds &lt;- apply(probs, 1, which.max)
      levels(self$Y)[preds]
    },
    compute_loss = function(probs = self$output) {
      Yid &lt;- match(self$Y, sort(unique(self$Y)))
      correct_logprobs &lt;- -log(probs[cbind(seq_along(Yid), Yid)])
      sum(correct_logprobs)
    },
    train = function(iterations = 1e4,
                     learn_rate = 1e-2,
                     tolerance = .01,
                     trace = 100) {
      for (i in seq_len(iterations)) {
        self$feedforward()$backpropagate(learn_rate)
        if (trace &gt; 0 &amp;&amp; i %% trace == 0)
          message(&#39;Iteration &#39;, i, &#39;\tLoss &#39;, self$compute_loss(),
                  &#39;\tAccuracy &#39;, self$accuracy())
        if (self$compute_loss() &lt; tolerance) break
      }
      invisible(self)
    },
    accuracy = function() {
      predictions &lt;- apply(self$output, 1, which.max)
      predictions &lt;- levels(self$Y)[predictions]
      mean(predictions == self$Y)
    },
    sigmoid = function(x) 1 / (1 + exp(-x)),
    dsigmoid = function(x) x * (1 - x),
    softmax = function(x) exp(x) / rowSums(exp(x))
  )
)</code></pre>
<p>What do each of the methods do?</p>
<ul>
<li><code>initialize</code> is run every time you make a new network. It initialises the weight matrices to be the correct dimensions, populated with random initial values.</li>
<li><code>fit</code> runs one step of forward propagation but returns the result instead of storing it. Useful for predicting from new test data without changing the model.</li>
<li><code>feedforward</code> runs <code>fit</code> but saves the results, for training.</li>
<li><code>backpropagate</code> does what you might expect, saving the results for training.</li>
<li><code>train</code> runs forward and back propagation, storing the results and printing the progress to the console every <code>trace</code> iterations.</li>
<li><code>predict</code> uses argmax to estimate a single discrete class for each observation (whereas <code>fit</code> only returns probabilities).</li>
<li><code>accuracy</code> and <code>compute_loss</code> return the proportion of correct class assignments and the negative log likelihood (‘log loss’), respectively</li>
<li><code>sigmoid</code>, <code>dsigmoid</code> and <code>softmax</code> are utility functions.</li>
</ul>
<p>Let’s see our fancy neural network in action on the famous <a href="https://en.wikipedia.org/wiki/Iris_flower_data_set">iris data set</a>.
Firstly, initialise the network by specifying the input and output variables and the number of hidden nodes.
(Let’s go for five.)</p>
<pre class="r"><code>irisnet &lt;- NeuralNetwork$new(Species ~ ., data = iris, hidden = 5)</code></pre>
<p>I have implemented a formula interface so I can regress <code>Species</code> on the other four variables (<code>Petal.Length</code>, <code>Petal.Width</code>, <code>Sepal.Length</code> and <code>Sepal.Width</code>) without having to type it out in full.</p>
<p>That’s the framework assembled.
Now we can train the model.
No need to assign any new variables—the R6 object modifies itself in place.</p>
<pre class="r"><code>irisnet$train(9999, trace = 1e3, learn_rate = .0001)
## Iteration 1000   Loss 151.054969487204   Accuracy 0.68
## Iteration 2000   Loss 81.6280605667542   Accuracy 0.853333333333333
## Iteration 3000   Loss 64.5735942975868   Accuracy 0.966666666666667
## Iteration 4000   Loss 51.0996158369837   Accuracy 0.973333333333333
## Iteration 5000   Loss 40.105949050699    Accuracy 0.973333333333333
## Iteration 6000   Loss 32.3280157122779   Accuracy 0.973333333333333
## Iteration 7000   Loss 27.0449301604686   Accuracy 0.98
## Iteration 8000   Loss 23.3961690114  Accuracy 0.98
## Iteration 9000   Loss 20.7875204792586   Accuracy 0.98</code></pre>
<p>You might notice that the log loss is going down even while accuracy stays the same.
This is because the model is becoming more or less confident about each of its predictions, even if the argmax doesn’t change.</p>
<p>Everything is stored inside the <code>NeuralNetwork</code> object.</p>
<pre class="r"><code>irisnet</code></pre>
<pre><code>## &lt;NeuralNetwork&gt;
##   Public:
##     accuracy: function () 
##     backpropagate: function (lr = 0.01) 
##     clone: function (deep = FALSE) 
##     compute_loss: function (probs = self$output) 
##     dsigmoid: function (x) 
##     feedforward: function (data = self$X) 
##     fit: function (data = self$X) 
##     initialize: function (formula, hidden, data = list()) 
##     output: 0.968263186096496 0.962577356981835 0.965919909020745 0. ...
##     predict: function (data = self$X) 
##     sigmoid: function (x) 
##     softmax: function (x) 
##     train: function (iterations = 10000, learn_rate = 0.01, tolerance = 0.01, 
##     W1: 0.535763605328154 1.04640352732849 1.11188605221437 -1.7 ...
##     W2: -0.412354248171205 2.04939691705817 -1.70485310730503 -3 ...
##     X: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1  ...
##     Y: setosa setosa setosa setosa setosa setosa setosa setosa  ...</code></pre>
<p>One other nice thing about R6 classes is you can chain methods together where they return <code>self</code> or <code>invisible(self)</code>.
So if I want to train for 1000 iterations and then predict for some new data, all in one line, it’s as simple as</p>
<pre class="r"><code>irisnet$train(1000)$predict(newdata)</code></pre>
<p>There are plenty of improvements we could make to the <code>NeuralNetwork</code> class but most of them are beyond the scope of this article, and left as an exercise to the interested reader.
Here are some ideas:</p>
<ul>
<li>user-selectable activation functions, rather than hard-coded sigmoid</li>
<li><a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)">regularisation</a>, and including regularisation loss as part of <code>compute_loss()</code></li>
<li>arbitrary number of hidden layers, to enable deep learning (multiple hidden layers) or logistic regression (no hidden layer)</li>
<li>regression as well as classification</li>
<li>more intelligent initial weights</li>
<li>changing learning rate over time</li>
</ul>
<p>But you might be wondering: can I actually classify pictures of hot dogs with this?
Probably not from raw pixels—image analysis is usually a job for a <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">convolutional neural network</a>, which is a more advanced topic for another day.</p>
<p>For further reading, try <a href="http://www.deeplearningbook.org/">Chapter 6 of <em>Deep Learning</em></a> (2016) by Goodfellow, Bengio and Courville.</p>
<p>I hope you found this useful!
Let me know if anything is unclear.
Full source code for this post is available <a href="https://github.com/Selbosh/selbosh.github.io/blob/source/content/post/2018-01-09-neural-network.Rmd">on GitHub</a>.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>If you wanted to be very general, then you could say the bias <em>does</em> depend on the previous layer, but the respective weight is fixed at zero. Then the weighted sum would be zero and <span class="math inline">\(\sigma(0) = 1\)</span>, so you get a vector of ones.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>

    </section>
    
			<section class="comments">
				<div id="disqus_thread"></div>
<script type="text/javascript">

(function() {
    
    
    if (window.location.hostname == "localhost")
        return;

    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    var disqus_shortname = 'tea-stats';
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com/" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
			</section>
		
  </article>
</main>

      <footer id="footer">
        
        <a href="#">
          <img src="https://selbydavid.com/img/logo.svg"
               width="60"
               height=""
               alt="Return to top"
               title="Return to top">
        </a>
        
        © 2016–20 David Selby | <a href='https://github.com/Selbosh/hugo-tea'>Hugo Tea Theme</a>
      </footer>
    
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src="//yihui.org/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-11136457-6', 'auto');
ga('send', 'pageview');
</script>

  </body>
</html>

